{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform LDA using Gensim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc1 = '!\"#$%\\'()*.:;<=>?@[\\\\]^`{|}~’“”‘–-' # adapted string.punctuation\n",
    "punc2 = ['=', '/', '&', '_', '+', '…', '...']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords list \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "additional_list = ['doin','nn','rd','st','wheres','hows','clean','theyve', 'weve', 'youve', 'de','u', 'yer', 'stuff','cause','mhm', 'mmhm','itit', 'youyou', 'ah', 'ifif', 'there', 'kinda', 'le','ill', 'hell', 'shell', 'whats', 'isnt', 'thats', 'theyve', 'arent', 'couldnt', 'didnt', 'hadnt', 'hasnt', 'werent', 'havent','dont', 'wont', 'cant', 'wouldnt', 'id', 'ive', 'gonna', 'hed', 'shouldnt', 'ii','dr','cuz','im','youre', 'hes', 'shes', 'were', 'theyre', 'thethe','theyll', 'youll', 'andand', 'th', 'thatthat', 'sthat', 'wewe','ti','u', 'heh', 'le', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'ya','nd', 'uhh', 's','d','t','by', 'don', 're', 'oh', 'ah', 'ahh', 'm', 'ok', 'okay', 'md', 'like','uh','uhum', 'go', 'got', 'yeah', 'okay', 'yep','uhm', 'umm', 'hum', 'na', 'um', 'legend', 'hmm', 'ah', 'na', 'mm', 'mmm', 'da', 'mmhmm', 'mmmhmm', 'yup', 'hm', 'know', 'would', 'get', 'other', 'huh']\n",
    "stop_words.extend(additional_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help with processing text\n",
    "def remove_parentheses(txt):\n",
    "    txt = re.sub('\\([^)]*\\)\\)','', txt) # remove double parentheses \n",
    "    txt = re.sub(r'\\([^)]*\\)', '', txt) # remove single parentheses \n",
    "    return txt\n",
    "\n",
    "def remove_numerical(txt):\n",
    "    txt = re.sub('[0-9]+', '', txt)\n",
    "    return txt\n",
    "\n",
    "def remove_punc(txt):\n",
    "    for a in punc2:\n",
    "        txt = txt.replace(a,\" \")\n",
    "    for b in punc1:\n",
    "        txt = txt.replace(b,\"\")\n",
    "    return txt\n",
    "\n",
    "def lowercase(txt):\n",
    "    txt = txt.lower()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_numerical('I lov3 food. I 8 a pi55a for lunch.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help with tokenizing, cleaning up text\n",
    "def sent_to_words(document):\n",
    "    return gensim.utils.simple_preprocess(str(document), deacc=True) # deacc=True removes punctuations\n",
    "\n",
    "def remove_stopwords(document):\n",
    "    return [word for word in document if word not in stop_words]\n",
    "\n",
    "def lemma_tokens(tokens, lemmatizer = WordNetLemmatizer()):\n",
    "    lemmed = []\n",
    "    for item in tokens:\n",
    "        lemmed.append(lemmatizer.lemmatize(item))\n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_transcripts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df # in case we need to revisit later on, we have a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are cleaning up each of the conversations in Convo_1 column and then tokenizing the texts\n",
    "conversations_tokenized = [] \n",
    "for index, row in df.iterrows():\n",
    "    if pd.isnull(row['Convo_1']) == False:\n",
    "        # remove parentheses, numbers, punctuation, and convert everything to lowercase\n",
    "        row['Convo_1'] = remove_parentheses(row['Convo_1'])\n",
    "        row['Convo_1'] = remove_numerical(row['Convo_1'])\n",
    "        row['Convo_1'] = remove_punc(row['Convo_1'])\n",
    "        row['Convo_1'] = lowercase(row['Convo_1'])\n",
    "        df.set_value(index,'Convo_1', row['Convo_1'])\n",
    "        conversations_tokenized.append(sent_to_words(row['Convo_1'])) # create new column of processed, tokenized doucments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "conversations_nostops = remove_stopwords(conversations_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the words\n",
    "conversations_lemmatized = []\n",
    "for doc in conversations_nostops:\n",
    "    conversations_lemmatized.append(lemma_tokens(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(conversations_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Term Document Frequencies\n",
    "corpus = [id2word.doc2bow(text) for text in conversations_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Dictionary based on the following criteria\n",
    "id2word.filter_extremes(no_below=3, no_above = 0.75, keep_n=7000, keep_tokens=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BOW Model\n",
    "corpus = [id2word.doc2bow(text) for text in conversations_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=15, \n",
    "                                           random_state=100,\n",
    "                                           update_every=2,\n",
    "                                           chunksize=25,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=conversations_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how the various parameters impact coherence score (see how each parameter impacts in isolation versus more of a grid-search approach)\n",
    "\n",
    "Brief non-technical explanation of topic coherence: https://www.quora.com/What-is-topic-coherence\n",
    "\n",
    "### No_below\n",
    "- Attempt 1: Coherence Score = 0.3878\n",
    "- Attempt 2: no_below = 2, Coherence Score = 0.4577\n",
    "- Attempt 3: no_below = 3, Coherence Score = 0.4577\n",
    "- Attempt 4: no_below = 5, Coherence Score = 0.3878\n",
    "- no_above = 0.8, keep_n = 5000, chunk_size = 50, update_every = 1, passes = 10\n",
    "\n",
    "### No_above\n",
    "- Attempt 1: Coherence Score = 0.45206915803222947\n",
    "- Attempt 2: no_above = 0.9, Coherence Score = 0.4372\n",
    "- Attempt 3: no_above = 0.8, Coherence Score = 0.4577\n",
    "- Attempt 4: no_above = 0.75, Coherence Score = 0.4611\n",
    "- Attempt 5: no_above = 0.70, Coherence Score = 0.4206\n",
    "- no_below = 3, keep_n = 5000, chunk_size = 50, update_every = 1, passes = 10\n",
    "\n",
    "### Keep_n\n",
    "- Attempt 1: keep_n = 3000, Coherence Score:  0.4555\n",
    "- Attempt 2: keep_n = 5000, Coherence Score:  0.4611\n",
    "- Attempt 3: keep_n = 7000, Coherence Score:  0.4747\n",
    "- no_below = 3, no_above = 0.75, chunk_size = 50, update_every = 1, passes = 10\n",
    "\n",
    "### Num_passes\n",
    "- Attempt 1: Passes = 5, Coherence Score = 0.4347\n",
    "- Attempt 2: Passes = 10, Coherence Score = 0.4819\n",
    "- Attempt 3: Passes = 20, Coherence Score = 0.4965\n",
    "- no_below = 3, no_above = 0.75, keep_n = 7000, chunk_size = 50, update_every = 1\n",
    "\n",
    "- Attempt 1: Passes = 5, Coherence Score = 0.4627\n",
    "- Attempt 2: Passes = 10, Coherence Score = 0.4747\n",
    "- Attempt 3: Passes = 20, Coherence Score = 0.5073\n",
    "- no_below = 3, no_above = 0.75, keep_n = 7000, chunk_size = 25, update_every = 2\n",
    "\n",
    "### Num_topics\n",
    "- Attempt 1: num_topics = 5, Coherence Score = 0.3472\n",
    "- Attempt 2: num_topics = 10, Coherence Score = 0.4747\n",
    "- Attempt 3: num_topics = 15, Coherence Score = 0.3778\n",
    "- no_below = 3, no_above = 0.75, keep_n = 7000, chunk_size = 25, update_every = 2, passes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, we would want non-intersecting topics (bubbles) in different coordinates. The size of the bubble represents the prevalence of the topic, and we would expect some to be larger than others (although we want them to be generally large because that means that each topic is important, versus having many small bubbles). However, we mostly see overlapping topics of the same size in our topic models, which means that the topic model did not identify very distinct topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting Word Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    for word in ['yer']: # de\n",
    "        if pd.isnull(row['Convo_1']) == False and word in gensim.utils.simple_preprocess(str(row['Convo_1']), deacc=True):\n",
    "            print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
