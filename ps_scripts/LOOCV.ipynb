{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/preethiseshadri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import NormModel\n",
    "\n",
    "#spacy for lemmatization\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.preprocessing import OneHotEncoder, scale, LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, LeaveOneOut\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_loo(clf, X_train, y_train, X_test, y_test):\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    y_pred_tr = clf.predict(X_train)\n",
    "    y_pred_ts = clf.predict(X_test)\n",
    "    return y_pred_tr, y_pred_ts\n",
    "\n",
    "def compute_metrics(actual, pred):\n",
    "    accuracy = metrics.accuracy_score(actual, pred)\n",
    "    precision = metrics.precision_score(actual, pred)\n",
    "    recall = metrics.recall_score(actual, pred)\n",
    "    auc = metrics.roc_auc_score(actual, pred)\n",
    "    return accuracy, precision, recall, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopWords] for doc in texts]\n",
    "\n",
    "def make_bigrams_dvd(texts):\n",
    "    return [bigram_mod_dvd[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams_dvd(texts):\n",
    "    return [trigram_mod_dvd[bigram_mod_dvd[doc]] for doc in texts]\n",
    "\n",
    "def make_bigrams_va(texts):\n",
    "    return [bigram_mod_va[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams_dvd(texts):\n",
    "    return [trigram_mod_va[bigram_mod_va[doc]] for doc in texts]\n",
    "\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 432 stop words.\n"
     ]
    }
   ],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "stopWords = set([word.replace(\"'\", \"\") for word in stopWords])\n",
    "stopWords = stopWords.union(set([\"taiwan\", \"taiwanese\", \"communist\", \"mmmhmm\", \"'\", \"'cause\", \"'em\", 'a', 'aa', 'aaah', 'aah', 'ab', 'about', 'above', 'african', 'after', 'again', 'against', 'ah', 'ahh', 'ahhh', 'ahhhh', 'ahhm', 'ain', 'aint', 'alabama', 'alaska', 'all', 'alot', 'alright', 'alrighty', 'also', 'am', 'an', 'anand', 'and', 'andand', 'any', 'anyone', 'are', 'aren', 'arent', 'as', 'at', 'ay', 'b', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'bye', 'c', 'california', 'came', 'can', 'cant', 'clean', 'costa_rica', 'could', 'couldn', 'couldnt', 'cuz', 'd', 'de', 'did', 'didn', 'didnt', 'do', 'doc', 'does', 'doesn', 'doesnt', 'doin', 'doing', 'dokey', 'don', 'dont', 'down', 'during', 'e', 'each', 'eek', 'eh', 'em', 'er', 'et', 'etc', 'europe', 'f', 'few', 'florida','for', 'from', 'further', 'g', 'ga', 'gal', 'gee', 'geez', 'germany', 'get', 'go', 'goin', 'going', 'gonna', 'gosh', 'got', 'gotta', 'greek', 'gu', 'h', 'ha', 'had', 'hadn', 'hadnt', 'has', 'hasn', 'hasnt', 'have', 'haven', 'havent', 'having', 'he', 'hed', 'heh', 'hell', 'hello', 'henry', 'her', 'here', 'hers', 'herself', 'hes', 'hey', 'hi', 'him', 'himself', 'his', 'hm', 'hmm', 'hmmm', 'hodgkins', 'how', 'hows', 'huh', 'hum', 'i', 'id', 'if', 'ifif', 'ii', 'iii', 'ill', 'im', 'imrt', 'in', 'inaudible', 'indecipherable', 'indianapolis', 'into', 'is', 'isis', 'isn', 'isnt', 'it', 'itd', 'itit', 'itll', 'its', 'itself', 'ive', 'j', 'jeez', 'just', 'k', 'kay', 'kinda', 'l', 'laughs', 'le', 'leastno', 'legend', 'let', 'lets', 'like', 'll', 'look', 'lot', 'm', 'ma', 'maam', 'md', 'mdmd', 'me', 'mhm', 'mhmm', 'mhmmm', 'michigan', 'mightn', 'mightnt', 'mightve', 'mkay', 'mm', 'mmhm', 'mmhmm', 'mmkay', 'mmm', 'mmmhmm','mmmhmmm', 'mmmm', 'mmmmm', 'more', 'most', 'mustn', 'mustnt', 'mustve', 'my', 'myself', 'n', 'na', 'nah', 'nahuh', 'nd', 'ne', 'needn', 'neednt', 'nn', 'no', 'nooh', 'noooo', 'nope', 'nor', 'not', 'now', 'o', 'of', 'off', 'oh', 'ohh', 'ohhh', 'ohhhohohohoh', 'ohio', 'ok', 'okay', 'okey', 'on', 'once', 'only', 'oooh', 'or', 'oth', 'other', 'othumhmm', 'oughta', 'our', 'ours', 'ourselves', 'out', 'over', 'ow', 'own', 'p', 'patient', 'phi', 'physician', 'potter', 'pt', 'pt/so', 'q', 'r', 'rd', 're', 'right', 'ro', 's', 'said', 'same', 'say', 'see', 'shan', 'shant', 'she', 'shell', 'shes', 'should', 'shouldn', 'shouldnt', 'shouldve', 'so', 'some', 'sorta', 'sounds', 'st', 'stuff', 'such', 'swedish', 't', 'th', 'than', 'that', 'thatd', 'thatll', 'thats', 'thatsthat', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'thered', 'thereof', 'theres', 'thereve', 'these', 'thethe', 'thew', 'they', 'theyll', 'theyre', 'theyve', 'thing', 'things', 'this', 'those', 'through', 'ti', 'to', 'too', 'tthe', 'u', 'uh', 'uhh', 'uhhhhh', 'uhhm', 'uhhmm', 'uhhuh', 'uhm', 'uhmhmm', 'uhmhmmm', 'uhmmm', 'uhoh', 'uhum', 'um', 'umhmm', 'umhmmm', 'umm', 'ummm', 'ummmm', 'un', 'under', 'unhunh', 'until', 'up', 'us', 'uuh', 'v', 've', 'very', 'vietnam', 'virginia', 'w', 'walsh', 'wanna', 'was', 'washington', 'wasn', 'wasnt', 'we', 'wed', 'well', 'went', 'were', 'weren', 'werent', 'weve', 'wewe', 'what', 'whatd', 'whatev', 'whatever', 'whatnot', 'whats', 'when', 'where', 'wheres', 'whew', 'which', 'while', 'who', 'whoa', 'whom', 'whos', 'why', 'will', 'with', 'won', 'wont', 'would', 'wouldn', 'wouldnt', 'x', 'y', 'ya', 'yada', 'yah', 'yall', 'yea', 'yeah', 'yep', 'yepvery', 'yer', 'yeyeah', 'you', 'youd', 'youl', 'youll', 'your', 'youre', 'yours', 'yourself', 'yourselves', 'youve', 'youyou', 'yup', 'z']))\n",
    "print(\"We have\", len(stopWords), \"stop words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline + Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dvd_withAdvice_final.csv')\n",
    "\n",
    "# Subset the dataframe\n",
    "lda_models = []\n",
    "factors_all = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"Convo_1\", \"as1\", \"sur1\", \"rad1\"]\n",
    "factors_sub = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"Convo_1\"]\n",
    "df = df.dropna(subset=factors_all)\n",
    "X = df[factors_sub]\n",
    "y = df[\"txgot_binary\"]\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the transcripts\n",
    "convo_dvd = df[\"Convo_1\"]\n",
    "data_words_dvd = list(sent_to_words(convo_dvd))\n",
    "bigram_dvd = gensim.models.Phrases(data_words_dvd, min_count=2, threshold=100) \n",
    "bigram_mod_dvd = gensim.models.phrases.Phraser(bigram_dvd)\n",
    "data_words_nostops_dvd = remove_stopwords(data_words_dvd)\n",
    "data_words_bigrams_dvd = make_bigrams_dvd(data_words_nostops_dvd)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "data_lemmatized_dvd = lemmatization(data_words_bigrams_dvd, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "data_lemmatized_dvd = [[word for word in convo if word not in stopWords] for convo in data_lemmatized_dvd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished round  131\n"
     ]
    }
   ],
   "source": [
    "# Run leave-one-out cross-validation\n",
    "predictions_ts = []\n",
    "accuracy_tr = []\n",
    "precision_tr = []\n",
    "recall_tr = []\n",
    "auc_tr = []\n",
    "max_depth = 3\n",
    "subsample = 1\n",
    "no_below = 0.1\n",
    "no_above = 0.95\n",
    "keep_n = 5000\n",
    "loo = LeaveOneOut()\n",
    "index = 1\n",
    "for train_index, test_index in loo.split(X):\n",
    "    #first split up the dataset\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Create Bag of words model\n",
    "    texts_lemmatized = np.array(data_lemmatized_dvd)\n",
    "    id2word_dvd = corpora.Dictionary(texts_lemmatized[train_index]) # build corpus on training data only\n",
    "    id2word_dvd.filter_extremes(no_below = no_below, no_above = no_above, keep_n = keep_n, keep_tokens = None)\n",
    "\n",
    "    corp_dvd = [id2word_dvd.doc2bow(text) for text in texts_lemmatized]\n",
    "    corp_dvd = np.array(corp_dvd)\n",
    "\n",
    "    # Split corpus\n",
    "    corp_dvd_train = corp_dvd[train_index]\n",
    "    corp_dvd_test = corp_dvd[test_index]\n",
    "\n",
    "    #LDA Model\n",
    "    lda_model_dvd = gensim.models.ldamodel.LdaModel(corpus=corp_dvd_train,\n",
    "                                           id2word=id2word_dvd,\n",
    "                                           num_topics=8, \n",
    "                                           random_state=100,\n",
    "                                           update_every=3,\n",
    "                                           chunksize=20,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    lda_models.append(lda_model_dvd)\n",
    "    \n",
    "    #convert testing and training to dataframe so can append distributions\n",
    "    X_train = pd.DataFrame({'age':X_train[:,0],'gleason':X_train[:,1], \"DVD\": X_train[:,2], \"TxChoice2_orig\": X_train[:, 3], 'Convo_1': X_train[:, 4]})\n",
    "    X_test = pd.DataFrame({'age':X_test[:,0],'gleason':X_test[:,1],\"DVD\": X_test[:,2], \"TxChoice2_orig\": X_test[:, 3], 'Convo_1': X_test[:, 4]})\n",
    "\n",
    "    # get training distributions\n",
    "    distributions = lda_model_dvd[corp_dvd_train]\n",
    "    dvd_length = len(corp_dvd_train) \n",
    "    topic0 = [0] * dvd_length\n",
    "    topic1 = [0] * dvd_length\n",
    "    topic2 = [0] * dvd_length\n",
    "    topic3 = [0] * dvd_length\n",
    "    topic4 = [0] * dvd_length\n",
    "    topic5 = [0] * dvd_length\n",
    "    topic6 = [0] * dvd_length\n",
    "    topic7 = [0] * dvd_length\n",
    "    # store the topic percentage values for training\n",
    "    for en, row in enumerate(distributions):\n",
    "        topics = row[0]\n",
    "        for topic in topics:\n",
    "            if topic[0] == 0:\n",
    "                topic0[en] = topic[1]\n",
    "            elif topic[0] == 1:\n",
    "                topic1[en] = topic[1]\n",
    "            elif topic[0] == 2:\n",
    "                topic2[en] = topic[1]\n",
    "            elif topic[0] == 3:\n",
    "                topic3[en] = topic[1]\n",
    "            elif topic[0] == 4:\n",
    "                topic4[en] = topic[1]\n",
    "            elif topic[0] == 5:\n",
    "                topic5[en] = topic[1]\n",
    "            elif topic[0] == 6:\n",
    "                topic6[en] = topic[1]\n",
    "            elif topic[0] == 7:\n",
    "                topic7[en] = topic[1]\n",
    "    X_train['topic0'] = topic0\n",
    "    X_train['topic1'] = topic1\n",
    "    X_train['topic2'] = topic2\n",
    "    X_train['topic3'] = topic3\n",
    "    X_train['topic4'] = topic4\n",
    "    X_train['topic5'] = topic5\n",
    "    X_train['topic6'] = topic6\n",
    "    X_train['topic7'] = topic7\n",
    "\n",
    "    # get testing distributions\n",
    "    distributions = lda_model_dvd[corp_dvd_test]\n",
    "    dvd_length = len(corp_dvd_test) \n",
    "    topic0 = [0] * dvd_length\n",
    "    topic1 = [0] * dvd_length\n",
    "    topic2 = [0] * dvd_length\n",
    "    topic3 = [0] * dvd_length\n",
    "    topic4 = [0] * dvd_length\n",
    "    topic5 = [0] * dvd_length\n",
    "    topic6 = [0] * dvd_length\n",
    "    topic7 = [0] * dvd_length\n",
    "    # store the topic percentage values for testing\n",
    "    for en, row in enumerate(distributions):\n",
    "        topics = row[0]\n",
    "        for topic in topics:\n",
    "            if topic[0] == 0:\n",
    "                topic0[en] = topic[1]\n",
    "            elif topic[0] == 1:\n",
    "                topic1[en] = topic[1]\n",
    "            elif topic[0] == 2:\n",
    "                topic2[en] = topic[1]\n",
    "            elif topic[0] == 3:\n",
    "                topic3[en] = topic[1]\n",
    "            elif topic[0] == 4:\n",
    "                topic4[en] = topic[1]\n",
    "            elif topic[0] == 5:\n",
    "                topic5[en] = topic[1]\n",
    "            elif topic[0] == 6:\n",
    "                topic6[en] = topic[1]\n",
    "            elif topic[0] == 7:\n",
    "                topic7[en] = topic[1]\n",
    "    X_test['topic0'] = topic0\n",
    "    X_test['topic1'] = topic1\n",
    "    X_test['topic2'] = topic2\n",
    "    X_test['topic3'] = topic3\n",
    "    X_test['topic4'] = topic4\n",
    "    X_test['topic5'] = topic5\n",
    "    X_test['topic6'] = topic6\n",
    "    X_test['topic7'] = topic7\n",
    "\n",
    "    X_train = X_train[[\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"topic0\", \"topic1\", \"topic2\", \"topic3\", \"topic4\", \"topic5\", \"topic6\", \"topic7\"]]\n",
    "    X_test = X_test[[\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"topic0\", \"topic1\", \"topic2\", \"topic3\", \"topic4\", \"topic5\", \"topic6\", \"topic7\"]]\n",
    " \n",
    "    xgb = XGBClassifier(max_depth = max_depth, subsample = subsample)\n",
    "    y_pred_tr, y_pred_ts = predict_loo(xgb, X_train, y_train, X_test, y_test)\n",
    "    predictions_ts.append(y_pred_ts)\n",
    "    \n",
    "    # training\n",
    "    acc, prec, rec, auc = compute_metrics(y_train, y_pred_tr)\n",
    "    accuracy_tr.append(acc)\n",
    "    precision_tr.append(prec)\n",
    "    recall_tr.append(rec)\n",
    "    auc_tr.append(auc)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('finished round ', index)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:  0.985613623018203\n",
      "training precision:  0.9955818504173335\n",
      "training recall:  0.9414144073001667\n",
      "training auc:  0.9700972724279535\n",
      "testing accuracy:  0.8244274809160306\n",
      "testing precision:  0.6206896551724138\n",
      "testing recall:  0.6\n",
      "testing auc  0.7455445544554457\n"
     ]
    }
   ],
   "source": [
    "# training performance\n",
    "print('training accuracy: ', np.average(accuracy_tr))\n",
    "print('training precision: ', np.average(precision_tr))\n",
    "print('training recall: ', np.average(recall_tr))\n",
    "print('training auc: ', np.average(auc_tr))\n",
    "\n",
    "# testing performance\n",
    "accuracy, precision, recall, auc = compute_metrics(y, predictions_ts)\n",
    "print('testing accuracy: ', accuracy)\n",
    "print('testing precision: ', precision)\n",
    "print('testing recall: ', recall)\n",
    "print('testing auc ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Convo_1 Performance\n",
    "\n",
    "Max_depth = 3, No_below = 0.1, No_above = 1, keep_n = 5000\n",
    "+ training accuracy: 0.991, training precision: 0.995, training recall: 0.967, training auc 0.983\n",
    "+ testing accuracy: 0.756, testing precision: 0.464, testing recall: 0.433, testing auc: 0.642\n",
    "\n",
    "Max_depth = 3, No_below = 0.1, No_above = 0.9, keep_n = 5000\n",
    "+ training accuracy:  0.985, training precision:  0.996, training recall:  0.940, training auc:  0.9694231\n",
    "+ testing accuracy:  0.794, testing precision:  0.556, testing recall:  0.5, testing auc  0.691\n",
    "\n",
    "Max_depth = 3, subsample = 0.9, no_below = 0.1, no_above = 0.75, keep_n = 5000\n",
    "+ training accuracy:  0.994, training precision:  0.999, training recall:  0.974, training auc:  0.987\n",
    "+ testing accuracy:  0.794, testing precision:  0.579, testing recall:  0.367, testing auc  0.644\n",
    "\n",
    "Max_depth = 3, No_below = 0.1, No_above = 0.95, keep_n = 7000\n",
    "+ training accuracy:  0.988, training precision:  0.999, training recall:  0.950, training auc:  0.975\n",
    "+ testing accuracy:  0.771, testing precision:  0.5, testing recall:  0.467, testing auc  0.664\n",
    "\n",
    "Max_depth = 3, Subsample = 0.9, No_below = 0.1, No_above = 0.95, Keep_n = 5000\n",
    "+ training accuracy:  0.9863769817968291, training precision:  0.997289621674859, training recall:  0.9432131262612968, training auc:  0.9712248837413355, testing accuracy:  0.816793893129771, testing precision:  0.6071428571428571, testing recall:  0.5666666666666667, testing auc  0.7288778877887788\n",
    "\n",
    "Max_depth = 3, No_below = 0.1, No_above = 0.95, Keep_n = 5000\n",
    "+ training accuracy:  0.986, training precision:  0.996, training recall:  0.942, training auc:  0.970\n",
    "+ testing accuracy:  0.824, testing precision:  0.621, testing recall:  0.6, testing auc  0.746\n",
    "\n",
    "Max_depth = 3, No_below = 0.1, No_above = 0.95, Keep_n = 4000\n",
    "+ training accuracy:  0.993, training precision:  1.0, training recall:  0.969, training auc:  0.984\n",
    "+ testing accuracy:  0.802, testing precision:  0.583, testing recall:  0.467, testing auc  0.684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline + Advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dvd_withAdvice_final.csv')\n",
    "\n",
    "# Subset the dataframe\n",
    "lda_models = []\n",
    "factors_all = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\",\"Convo_1\", \"as1\", \"sur1\", \"rad1\"]\n",
    "factors_sub = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\",\"as1\", \"sur1\", \"rad1\"]\n",
    "df = df.dropna(subset=factors_all)\n",
    "X = df[factors_sub]\n",
    "y = df[\"txgot_binary\"]\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Run leave-one-out cross-validation\n",
    "predictions_ts = []\n",
    "accuracy_tr = []\n",
    "precision_tr = []\n",
    "recall_tr = []\n",
    "auc_tr = []\n",
    "max_depth = 3\n",
    "loo = LeaveOneOut()\n",
    "index = 1\n",
    "for train_index, test_index in loo.split(X):\n",
    "    #first split up the dataset\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = pd.DataFrame({'age':X_train[:,0],'gleason':X_train[:,1], \"DVD\": X_train[:,2], \"TxChoice2_orig\": X_train[:, 3], 'as1': X_train[:, 4], 'sur1': X_train[:, 5], 'rad1': X_train[:, 6]})\n",
    "    X_test = pd.DataFrame({'age':X_test[:,0],'gleason':X_test[:,1], \"DVD\": X_test[:,2], \"TxChoice2_orig\": X_test[:, 3], 'as1': X_test[:, 4], 'sur1': X_test[:, 5], 'rad1': X_test[:, 6]})\n",
    "    \n",
    "    X_train = X_train[[\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"as1\", \"sur1\", \"rad1\"]]\n",
    "    X_test = X_test[[\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"as1\", \"sur1\", \"rad1\"]]\n",
    " \n",
    "    xgb = XGBClassifier(max_depth = max_depth)\n",
    "    y_pred_tr, y_pred_ts = predict_loo(xgb, X_train, y_train, X_test, y_test)\n",
    "    predictions_ts.append(y_pred_ts)\n",
    "    \n",
    "    # training\n",
    "    acc, prec, rec, auc = compute_metrics(y_train, y_pred_tr)\n",
    "    accuracy_tr.append(acc)\n",
    "    precision_tr.append(prec)\n",
    "    recall_tr.append(rec)\n",
    "    auc_tr.append(auc)\n",
    "    \n",
    "    print('finished round ', index)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training performance\n",
    "print('training accuracy: ', np.average(accuracy_tr))\n",
    "print('training precision: ', np.average(precision_tr))\n",
    "print('training recall: ', np.average(recall_tr))\n",
    "print('training auc: ', np.average(auc_tr))\n",
    "\n",
    "# testing performance\n",
    "accuracy, precision, recall, auc = compute_metrics(y, predictions_ts)\n",
    "print('testing accuracy: ', accuracy)\n",
    "print('testing precision: ', precision)\n",
    "print('testing recall: ', recall)\n",
    "print('testing auc ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline + Advice Performance\n",
    "\n",
    "Max_depth = 3\n",
    "+ training accuracy:  0.9619495008807987, training precision:  0.92926528531484, training recall:  0.9028428533824692, training auc:  0.941176169341072\n",
    "+ testing accuracy:  0.8091603053435115, testing precision:  0.5925925925925926, testing recall:  0.5333333333333333, testing auc  0.7122112211221121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dvd_withAdvice_final.csv')\n",
    "\n",
    "# Subset the dataframe\n",
    "lda_models = []\n",
    "factors_all = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\",\"Convo_1\", \"as1\", \"sur1\", \"rad1\"]\n",
    "factors_sub = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\"]\n",
    "df = df.dropna(subset=factors_all)\n",
    "X = df[factors_sub]\n",
    "y = df[\"txgot_binary\"]\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print('Shape :', X.shape)\n",
    "\n",
    "# Run leave-one-out cross-validation\n",
    "predictions_ts = []\n",
    "accuracy_tr = []\n",
    "precision_tr = []\n",
    "recall_tr = []\n",
    "auc_tr = []\n",
    "max_depth = 3\n",
    "loo = LeaveOneOut()\n",
    "index = 1\n",
    "for train_index, test_index in loo.split(X):\n",
    "    #first split up the dataset\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = pd.DataFrame({'age':X_train[:,0],'gleason':X_train[:,1], \"DVD\": X_train[:,2], \"TxChoice2_orig\" : X_train[:, 3]})\n",
    "    X_test = pd.DataFrame({'age':X_test[:,0],'gleason':X_test[:,1], \"DVD\": X_test[:,2], \"TxChoice2_orig\" : X_test[:, 3]})\n",
    "    \n",
    "    X_train = X_train[[\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\"]]\n",
    "    X_test = X_test[[\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\"]]\n",
    "    \n",
    "    xgb = XGBClassifier(max_depth = max_depth)\n",
    "    y_pred_tr, y_pred_ts = predict_loo(xgb, X_train, y_train, X_test, y_test)\n",
    "    predictions_ts.append(y_pred_ts)\n",
    "    \n",
    "    # training\n",
    "    acc, prec, rec, auc = compute_metrics(y_train, y_pred_tr)\n",
    "    accuracy_tr.append(acc)\n",
    "    precision_tr.append(prec)\n",
    "    recall_tr.append(rec)\n",
    "    auc_tr.append(auc)\n",
    "    \n",
    "    print('finished round ', index)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training performance\n",
    "print('training accuracy: ', np.average(accuracy_tr))\n",
    "print('training precision: ', np.average(precision_tr))\n",
    "print('training recall: ', np.average(recall_tr))\n",
    "print('training auc: ', np.average(auc_tr))\n",
    "\n",
    "# testing performance\n",
    "accuracy, precision, recall, auc = compute_metrics(y, predictions_ts)\n",
    "print('testing accuracy: ', accuracy)\n",
    "print('testing precision: ', precision)\n",
    "print('testing recall: ', recall)\n",
    "print('testing auc ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Performance\n",
    "\n",
    "Max_depth = 3\n",
    "+ training accuracy:  0.8826189078097477, training precision:  0.8552019941132032, training recall:  0.5868737387031676, training auc:  0.7786602084793897\n",
    "+ testing accuracy:  0.7938931297709924, testing precision:  0.56, testing recall:  0.4666666666666667, testing auc:  0.6788778877887789"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dvd_withAdvice_final.csv')\n",
    "\n",
    "# Subset the dataframe\n",
    "lda_models = []\n",
    "factors_all = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"Convo_1\", \"as1\", \"sur1\", \"rad1\"]\n",
    "factors_sub = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"Convo_1\"]\n",
    "df = df.dropna(subset=factors_all)\n",
    "X = df[factors_sub]\n",
    "y = df[\"txgot_binary\"]\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Remove stopwords, create bigrams/trigrams, lemmatize everything\n",
    "convo_dvd = df[\"Convo_1\"]\n",
    "data_words_dvd = list(sent_to_words(convo_dvd))\n",
    "bigram_dvd = gensim.models.Phrases(data_words_dvd, min_count=2, threshold=100) \n",
    "bigram_mod_dvd = gensim.models.phrases.Phraser(bigram_dvd)\n",
    "data_words_nostops_dvd = remove_stopwords(data_words_dvd)\n",
    "data_words_bigrams_dvd = make_bigrams_dvd(data_words_nostops_dvd)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "data_lemmatized_dvd = lemmatization(data_words_bigrams_dvd, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "data_lemmatized_dvd = [[word for word in convo if word not in stopWords] for convo in data_lemmatized_dvd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "max_depth = 3\n",
    "subsample = 1\n",
    "no_below = 0.1\n",
    "no_above = 0.95\n",
    "keep_n = 5000\n",
    "\n",
    "# Build Corpus\n",
    "texts_lemmatized = np.array(data_lemmatized_dvd)\n",
    "id2word_dvd = corpora.Dictionary(texts_lemmatized) # build corpus on entire dataset\n",
    "id2word_dvd.filter_extremes(no_below = no_below, no_above = no_above, keep_n = keep_n, keep_tokens = None)\n",
    "corp_dvd = [id2word_dvd.doc2bow(text) for text in texts_lemmatized]\n",
    "corp_dvd = np.array(corp_dvd)\n",
    "\n",
    "# Create LDA Model with corpus from above\n",
    "lda_model_dvd = gensim.models.ldamodel.LdaModel(corpus=corp_dvd,\n",
    "                                       id2word=id2word_dvd,\n",
    "                                       num_topics=8, \n",
    "                                       random_state=100,\n",
    "                                       update_every=3,\n",
    "                                       chunksize=20,\n",
    "                                       passes=10,\n",
    "                                       alpha='auto',\n",
    "                                       per_word_topics=True)\n",
    "\n",
    "\n",
    "X = pd.DataFrame({'age':X[:,0],'gleason':X[:,1], \"DVD\": X[:,2], \"TxChoice2_orig\": X[:, 3], 'Convo_1': X[:, 4]})\n",
    "# Get topic distributions for entire dataset\n",
    "distributions = lda_model_dvd[corp_dvd]\n",
    "dvd_length = len(corp_dvd) \n",
    "topic0 = [0] * dvd_length\n",
    "topic1 = [0] * dvd_length\n",
    "topic2 = [0] * dvd_length\n",
    "topic3 = [0] * dvd_length\n",
    "topic4 = [0] * dvd_length\n",
    "topic5 = [0] * dvd_length\n",
    "topic6 = [0] * dvd_length\n",
    "topic7 = [0] * dvd_length\n",
    "# Store the topic percentage values\n",
    "for en, row in enumerate(distributions):\n",
    "    topics = row[0]\n",
    "    for topic in topics:\n",
    "        if topic[0] == 0:\n",
    "            topic0[en] = topic[1]\n",
    "        elif topic[0] == 1:\n",
    "            topic1[en] = topic[1]\n",
    "        elif topic[0] == 2:\n",
    "            topic2[en] = topic[1]\n",
    "        elif topic[0] == 3:\n",
    "            topic3[en] = topic[1]\n",
    "        elif topic[0] == 4:\n",
    "            topic4[en] = topic[1]\n",
    "        elif topic[0] == 5:\n",
    "            topic5[en] = topic[1]\n",
    "        elif topic[0] == 6:\n",
    "            topic6[en] = topic[1]\n",
    "        elif topic[0] == 7:\n",
    "            topic7[en] = topic[1]\n",
    "# Create new columns in dataframe with topic distributions\n",
    "X['topic0'] = topic0\n",
    "X['topic1'] = topic1\n",
    "X['topic2'] = topic2\n",
    "X['topic3'] = topic3\n",
    "X['topic4'] = topic4\n",
    "X['topic5'] = topic5\n",
    "X['topic6'] = topic6\n",
    "X['topic7'] = topic7\n",
    "\n",
    "# Subset dataframe for model training \n",
    "X = X[[\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"topic0\", \"topic1\", \"topic2\", \"topic3\", \"topic4\", \"topic5\", \"topic6\", \"topic7\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['topic0', 0.23295455]\n",
      "['age', 0.22537878]\n",
      "['topic4', 0.14204545]\n",
      "['topic1', 0.12310606]\n",
      "['topic7', 0.10227273]\n",
      "['gleason', 0.08901515]\n",
      "['TxChoice2_orig', 0.06439394]\n",
      "['topic2', 0.020833334]\n",
      "['topic6', 0.0]\n",
      "['topic5', 0.0]\n",
      "['topic3', 0.0]\n",
      "['DVD', 0.0]\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "clf = XGBClassifier(max_depth = max_depth)\n",
    "X_np = np.array(X)\n",
    "clf.fit(X_np, y)\n",
    "plot_importance(clf)\n",
    "pyplot.show()\n",
    "\n",
    "sorted_idx = np.argsort(clf.feature_importances_)[::-1]\n",
    "for index in sorted_idx:\n",
    "    print([X.columns[index], clf.feature_importances_[index]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DVD Topics:\n",
      "[(0,\n",
      "  '0.009*\"kind\" + 0.009*\"check\" + 0.008*\"feel\" + 0.008*\"erection\" + '\n",
      "  '0.006*\"help\" + 0.006*\"psa\" + 0.006*\"guy\" + 0.006*\"everything\" + '\n",
      "  '0.006*\"control\" + 0.006*\"work\" + 0.006*\"remove\" + 0.005*\"recovery\" + '\n",
      "  '0.005*\"doctor\" + 0.005*\"still\" + 0.005*\"side\" + 0.005*\"pretty\" + '\n",
      "  '0.005*\"bladder\" + 0.005*\"surveillance\" + 0.005*\"young\" + 0.005*\"use\" + '\n",
      "  '0.004*\"catheter\" + 0.004*\"nice\" + 0.004*\"fine\" + 0.004*\"meet\" + '\n",
      "  '0.004*\"high\" + 0.004*\"urinary\" + 0.004*\"ahead\" + 0.004*\"robotic\" + '\n",
      "  '0.004*\"decision\" + 0.004*\"sometimes\"'),\n",
      " (1,\n",
      "  '0.012*\"risk\" + 0.010*\"kind\" + 0.009*\"treat\" + 0.008*\"psa\" + '\n",
      "  '0.008*\"actually\" + 0.007*\"high\" + 0.007*\"put\" + 0.006*\"low\" + 0.006*\"use\" + '\n",
      "  '0.006*\"urinary\" + 0.006*\"different\" + 0.006*\"gleason\" + 0.006*\"work\" + '\n",
      "  '0.006*\"side_effect\" + 0.005*\"start\" + 0.005*\"try\" + 0.005*\"even\" + '\n",
      "  '0.005*\"gland\" + 0.005*\"less\" + 0.005*\"still\" + 0.005*\"study\" + 0.004*\"dose\" '\n",
      "  '+ 0.004*\"seed\" + 0.004*\"maybe\" + 0.004*\"feel\" + 0.004*\"rectum\" + '\n",
      "  '0.004*\"type\" + 0.004*\"sort\" + 0.004*\"usually\" + 0.004*\"therapy\"'),\n",
      " (2,\n",
      "  '0.010*\"work\" + 0.008*\"recovery\" + 0.006*\"term\" + 0.006*\"control\" + '\n",
      "  '0.005*\"somebody\" + 0.005*\"nerve\" + 0.005*\"robotic\" + 0.005*\"mmhmmm\" + '\n",
      "  '0.005*\"catheter\" + 0.004*\"even\" + 0.004*\"still\" + 0.004*\"surgeon\" + '\n",
      "  '0.004*\"use\" + 0.004*\"else\" + 0.004*\"food\" + 0.003*\"big\" + 0.003*\"able\" + '\n",
      "  '0.003*\"pretty\" + 0.003*\"help\" + 0.003*\"incision\" + 0.003*\"cell\" + '\n",
      "  '0.003*\"aggressive\" + 0.003*\"try\" + 0.003*\"lift\" + 0.003*\"sometimes\" + '\n",
      "  '0.003*\"occasionally\" + 0.003*\"last\" + 0.003*\"reason\" + 0.003*\"find\" + '\n",
      "  '0.003*\"urinary\"'),\n",
      " (3,\n",
      "  '0.000*\"risk\" + 0.000*\"kind\" + 0.000*\"use\" + 0.000*\"work\" + 0.000*\"treat\" + '\n",
      "  '0.000*\"side_effect\" + 0.000*\"feel\" + 0.000*\"gleason\" + 0.000*\"low\" + '\n",
      "  '0.000*\"high\" + 0.000*\"check\" + 0.000*\"put\" + 0.000*\"still\" + 0.000*\"guy\" + '\n",
      "  '0.000*\"even\" + 0.000*\"psa\" + 0.000*\"urinary\" + 0.000*\"side\" + 0.000*\"study\" '\n",
      "  '+ 0.000*\"different\" + 0.000*\"big\" + 0.000*\"actually\" + 0.000*\"usually\" + '\n",
      "  '0.000*\"start\" + 0.000*\"maybe\" + 0.000*\"disease\" + 0.000*\"pretty\" + '\n",
      "  '0.000*\"information\" + 0.000*\"small\" + 0.000*\"couple\"'),\n",
      " (4,\n",
      "  '0.034*\"kind\" + 0.015*\"low\" + 0.010*\"risk\" + 0.008*\"nerve\" + 0.008*\"bladder\" '\n",
      "  '+ 0.007*\"psa\" + 0.007*\"actually\" + 0.006*\"walk\" + 0.006*\"exactly\" + '\n",
      "  '0.006*\"pretty\" + 0.006*\"still\" + 0.006*\"put\" + 0.006*\"even\" + 0.006*\"guess\" '\n",
      "  '+ 0.006*\"chance\" + 0.006*\"feel\" + 0.005*\"great\" + 0.005*\"keep\" + '\n",
      "  '0.005*\"cell\" + 0.005*\"last\" + 0.005*\"active_surveillance\" + 0.005*\"usually\" '\n",
      "  '+ 0.005*\"number\" + 0.005*\"urine\" + 0.005*\"sideeffect\" + 0.005*\"procedure\" + '\n",
      "  '0.005*\"gleason\" + 0.004*\"maybe\" + 0.004*\"high\" + 0.004*\"cure\"'),\n",
      " (5,\n",
      "  '0.000*\"kind\" + 0.000*\"use\" + 0.000*\"risk\" + 0.000*\"high\" + 0.000*\"feel\" + '\n",
      "  '0.000*\"psa\" + 0.000*\"check\" + 0.000*\"low\" + 0.000*\"work\" + 0.000*\"start\" + '\n",
      "  '0.000*\"even\" + 0.000*\"side\" + 0.000*\"usually\" + 0.000*\"disease\" + '\n",
      "  '0.000*\"issue\" + 0.000*\"pretty\" + 0.000*\"catheter\" + 0.000*\"couple\" + '\n",
      "  '0.000*\"bladder\" + 0.000*\"maybe\" + 0.000*\"still\" + 0.000*\"less\" + '\n",
      "  '0.000*\"help\" + 0.000*\"side_effect\" + 0.000*\"wait\" + 0.000*\"actually\" + '\n",
      "  '0.000*\"different\" + 0.000*\"treat\" + 0.000*\"erection\" + 0.000*\"find\"'),\n",
      " (6,\n",
      "  '0.000*\"filter\" + 0.000*\"ward\" + 0.000*\"psa\" + 0.000*\"risk\" + 0.000*\"gift\" + '\n",
      "  '0.000*\"kind\" + 0.000*\"mistaken\" + 0.000*\"dad\" + 0.000*\"work\" + 0.000*\"feel\" '\n",
      "  '+ 0.000*\"high\" + 0.000*\"old\" + 0.000*\"aggressive\" + 0.000*\"gleason\" + '\n",
      "  '0.000*\"guy\" + 0.000*\"still\" + 0.000*\"maybe\" + 0.000*\"start\" + 0.000*\"use\" + '\n",
      "  '0.000*\"walking\" + 0.000*\"actually\" + 0.000*\"live\" + 0.000*\"wait\" + '\n",
      "  '0.000*\"side_effect\" + 0.000*\"check\" + 0.000*\"even\" + 0.000*\"nice\" + '\n",
      "  '0.000*\"bladder\" + 0.000*\"side\" + 0.000*\"leave\"'),\n",
      " (7,\n",
      "  '0.016*\"disease\" + 0.010*\"work\" + 0.009*\"bladder\" + 0.008*\"usually\" + '\n",
      "  '0.008*\"try\" + 0.008*\"gleason\" + 0.008*\"core\" + 0.007*\"erection\" + '\n",
      "  '0.007*\"number\" + 0.007*\"treat\" + 0.006*\"base\" + 0.006*\"catheter\" + '\n",
      "  '0.006*\"psa\" + 0.006*\"kind\" + 0.006*\"stay\" + 0.006*\"incontinence\" + '\n",
      "  '0.005*\"tissue\" + 0.005*\"use\" + 0.005*\"wait\" + 0.005*\"sort\" + '\n",
      "  '0.005*\"aggressive\" + 0.005*\"check\" + 0.005*\"less\" + 0.005*\"understand\" + '\n",
      "  '0.004*\"term\" + 0.004*\"pretty\" + 0.004*\"still\" + 0.004*\"change\" + '\n",
      "  '0.004*\"heal\" + 0.004*\"depend\"')]\n"
     ]
    }
   ],
   "source": [
    "print(\"DVD Topics:\")\n",
    "pprint(lda_model_dvd.print_topics(num_words=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
