{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/preethiseshadri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "#gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import NormModel\n",
    "\n",
    "#spacy for lemmatization\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "from sklearn.preprocessing import OneHotEncoder, scale, LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, LeaveOneOut\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_loo(clf, X_train, y_train, X_test, y_test):\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    y_pred_tr = clf.predict(X_train)\n",
    "    y_pred_ts = clf.predict(X_test)\n",
    "    return y_pred_tr, y_pred_ts\n",
    "\n",
    "def compute_metrics(actual, pred):\n",
    "    accuracy = metrics.accuracy_score(actual, pred)\n",
    "    precision = metrics.precision_score(actual, pred)\n",
    "    recall = metrics.recall_score(actual, pred)\n",
    "    auc = metrics.roc_auc_score(actual, pred)\n",
    "    return accuracy, precision, recall, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopWords] for doc in texts]\n",
    "\n",
    "def make_bigrams_dvd(texts):\n",
    "    return [bigram_mod_dvd[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams_dvd(texts):\n",
    "    return [trigram_mod_dvd[bigram_mod_dvd[doc]] for doc in texts]\n",
    "\n",
    "def make_bigrams_va(texts):\n",
    "    return [bigram_mod_va[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams_dvd(texts):\n",
    "    return [trigram_mod_va[bigram_mod_va[doc]] for doc in texts]\n",
    "\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 432 stop words.\n"
     ]
    }
   ],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "stopWords = set([word.replace(\"'\", \"\") for word in stopWords])\n",
    "stopWords = stopWords.union(set([\"taiwan\", \"taiwanese\", \"communist\", \"mmmhmm\", \"'\", \"'cause\", \"'em\", 'a', 'aa', 'aaah', 'aah', 'ab', 'about', 'above', 'african', 'after', 'again', 'against', 'ah', 'ahh', 'ahhh', 'ahhhh', 'ahhm', 'ain', 'aint', 'alabama', 'alaska', 'all', 'alot', 'alright', 'alrighty', 'also', 'am', 'an', 'anand', 'and', 'andand', 'any', 'anyone', 'are', 'aren', 'arent', 'as', 'at', 'ay', 'b', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'bye', 'c', 'california', 'came', 'can', 'cant', 'clean', 'costa_rica', 'could', 'couldn', 'couldnt', 'cuz', 'd', 'de', 'did', 'didn', 'didnt', 'do', 'doc', 'does', 'doesn', 'doesnt', 'doin', 'doing', 'dokey', 'don', 'dont', 'down', 'during', 'e', 'each', 'eek', 'eh', 'em', 'er', 'et', 'etc', 'europe', 'f', 'few', 'florida','for', 'from', 'further', 'g', 'ga', 'gal', 'gee', 'geez', 'germany', 'get', 'go', 'goin', 'going', 'gonna', 'gosh', 'got', 'gotta', 'greek', 'gu', 'h', 'ha', 'had', 'hadn', 'hadnt', 'has', 'hasn', 'hasnt', 'have', 'haven', 'havent', 'having', 'he', 'hed', 'heh', 'hell', 'hello', 'henry', 'her', 'here', 'hers', 'herself', 'hes', 'hey', 'hi', 'him', 'himself', 'his', 'hm', 'hmm', 'hmmm', 'hodgkins', 'how', 'hows', 'huh', 'hum', 'i', 'id', 'if', 'ifif', 'ii', 'iii', 'ill', 'im', 'imrt', 'in', 'inaudible', 'indecipherable', 'indianapolis', 'into', 'is', 'isis', 'isn', 'isnt', 'it', 'itd', 'itit', 'itll', 'its', 'itself', 'ive', 'j', 'jeez', 'just', 'k', 'kay', 'kinda', 'l', 'laughs', 'le', 'leastno', 'legend', 'let', 'lets', 'like', 'll', 'look', 'lot', 'm', 'ma', 'maam', 'md', 'mdmd', 'me', 'mhm', 'mhmm', 'mhmmm', 'michigan', 'mightn', 'mightnt', 'mightve', 'mkay', 'mm', 'mmhm', 'mmhmm', 'mmkay', 'mmm', 'mmmhmm','mmmhmmm', 'mmmm', 'mmmmm', 'more', 'most', 'mustn', 'mustnt', 'mustve', 'my', 'myself', 'n', 'na', 'nah', 'nahuh', 'nd', 'ne', 'needn', 'neednt', 'nn', 'no', 'nooh', 'noooo', 'nope', 'nor', 'not', 'now', 'o', 'of', 'off', 'oh', 'ohh', 'ohhh', 'ohhhohohohoh', 'ohio', 'ok', 'okay', 'okey', 'on', 'once', 'only', 'oooh', 'or', 'oth', 'other', 'othumhmm', 'oughta', 'our', 'ours', 'ourselves', 'out', 'over', 'ow', 'own', 'p', 'patient', 'phi', 'physician', 'potter', 'pt', 'pt/so', 'q', 'r', 'rd', 're', 'right', 'ro', 's', 'said', 'same', 'say', 'see', 'shan', 'shant', 'she', 'shell', 'shes', 'should', 'shouldn', 'shouldnt', 'shouldve', 'so', 'some', 'sorta', 'sounds', 'st', 'stuff', 'such', 'swedish', 't', 'th', 'than', 'that', 'thatd', 'thatll', 'thats', 'thatsthat', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'thered', 'thereof', 'theres', 'thereve', 'these', 'thethe', 'thew', 'they', 'theyll', 'theyre', 'theyve', 'thing', 'things', 'this', 'those', 'through', 'ti', 'to', 'too', 'tthe', 'u', 'uh', 'uhh', 'uhhhhh', 'uhhm', 'uhhmm', 'uhhuh', 'uhm', 'uhmhmm', 'uhmhmmm', 'uhmmm', 'uhoh', 'uhum', 'um', 'umhmm', 'umhmmm', 'umm', 'ummm', 'ummmm', 'un', 'under', 'unhunh', 'until', 'up', 'us', 'uuh', 'v', 've', 'very', 'vietnam', 'virginia', 'w', 'walsh', 'wanna', 'was', 'washington', 'wasn', 'wasnt', 'we', 'wed', 'well', 'went', 'were', 'weren', 'werent', 'weve', 'wewe', 'what', 'whatd', 'whatev', 'whatever', 'whatnot', 'whats', 'when', 'where', 'wheres', 'whew', 'which', 'while', 'who', 'whoa', 'whom', 'whos', 'why', 'will', 'with', 'won', 'wont', 'would', 'wouldn', 'wouldnt', 'x', 'y', 'ya', 'yada', 'yah', 'yall', 'yea', 'yeah', 'yep', 'yepvery', 'yer', 'yeyeah', 'you', 'youd', 'youl', 'youll', 'your', 'youre', 'yours', 'yourself', 'yourselves', 'youve', 'youyou', 'yup', 'z']))\n",
    "print(\"We have\", len(stopWords), \"stop words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline + Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dvd_advice_lda.csv')\n",
    "\n",
    "# Subset the dataframe\n",
    "lda_models = []\n",
    "factors_all = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"Convo_1\", \"as1\", \"sur1\", \"rad1\"]\n",
    "factors_sub = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"Convo_1\"]\n",
    "df = df.dropna(subset=factors_all)\n",
    "X = df[factors_sub]\n",
    "y = df[\"txgot_binary\"]\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Process the transcripts\n",
    "convo_dvd = df[\"Convo_1\"]\n",
    "data_words_dvd = list(sent_to_words(convo_dvd))\n",
    "bigram_dvd = gensim.models.Phrases(data_words_dvd, min_count=2, threshold=100) \n",
    "trigram_dvd = gensim.models.Phrases(bigram_dvd[data_words_dvd], threshold=100)  \n",
    "bigram_mod_dvd = gensim.models.phrases.Phraser(bigram_dvd)\n",
    "trigram_mod_dvd = gensim.models.phrases.Phraser(trigram_dvd)\n",
    "data_words_nostops_dvd = remove_stopwords(data_words_dvd)\n",
    "data_words_bigrams_dvd = make_bigrams_dvd(data_words_nostops_dvd)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "data_lemmatized_dvd = lemmatization(data_words_bigrams_dvd, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "data_lemmatized_dvd = [[word for word in convo if word not in stopWords] for convo in data_lemmatized_dvd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run leave-one-out cross-validation\n",
    "predictions_ts = []\n",
    "accuracy_tr = []\n",
    "precision_tr = []\n",
    "recall_tr = []\n",
    "auc_tr = []\n",
    "max_depth = 3\n",
    "subsample = 0.9\n",
    "loo = LeaveOneOut()\n",
    "index = 1\n",
    "for train_index, test_index in loo.split(X):\n",
    "    #first split up the dataset\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Create Bag of words model\n",
    "    no_below = 0.1\n",
    "    no_above = 0.9\n",
    "    texts_lemmatized = np.array(data_lemmatized_dvd)\n",
    "    id2word_dvd = corpora.Dictionary(texts_lemmatized[train_index]) # build corpus on training data only\n",
    "    id2word_dvd.filter_extremes(no_below = no_below, no_above = no_above, keep_n = 5000, keep_tokens = None)\n",
    "\n",
    "    corp_dvd = [id2word_dvd.doc2bow(text) for text in texts_lemmatized]\n",
    "    corp_dvd = np.array(corp_dvd)\n",
    "\n",
    "    # Split corpus\n",
    "    corp_dvd_train = corp_dvd[train_index]\n",
    "    corp_dvd_test = corp_dvd[test_index]\n",
    "\n",
    "    #LDA Model\n",
    "    lda_model_dvd = gensim.models.ldamodel.LdaModel(corpus=corp_dvd_train,\n",
    "                                           id2word=id2word_dvd,\n",
    "                                           num_topics=12, \n",
    "                                           random_state=100,\n",
    "                                           update_every=3,\n",
    "                                           chunksize=20,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "    lda_models.append(lda_model_dvd)\n",
    "    \n",
    "    #convert testing and training to dataframe so can append distributions\n",
    "    X_train = pd.DataFrame({'age':X_train[:,0],'gleason':X_train[:,1], \"DVD\": X_train[:,2], \"TxChoice2_orig\": X_train[:, 3], 'Convo_1': X_train[:, 4]})\n",
    "    X_test = pd.DataFrame({'age':X_test[:,0],'gleason':X_test[:,1],\"DVD\": X_test[:,2], \"TxChoice2_orig\": X_test[:, 3], 'Convo_1': X_test[:, 4]})\n",
    "\n",
    "    # get training distributions\n",
    "    distributions = lda_model_dvd[corp_dvd_train]\n",
    "    dvd_length = len(corp_dvd_train) \n",
    "    topic0 = [0] * dvd_length\n",
    "    topic1 = [0] * dvd_length\n",
    "    topic2 = [0] * dvd_length\n",
    "    topic3 = [0] * dvd_length\n",
    "    topic4 = [0] * dvd_length\n",
    "    topic5 = [0] * dvd_length\n",
    "    topic6 = [0] * dvd_length\n",
    "    topic7 = [0] * dvd_length\n",
    "    # store the topic percentage values for training\n",
    "    for en, row in enumerate(distributions):\n",
    "        topics = row[0]\n",
    "        for topic in topics:\n",
    "            if topic[0] == 0:\n",
    "                topic0[en] = topic[1]\n",
    "            elif topic[0] == 1:\n",
    "                topic1[en] = topic[1]\n",
    "            elif topic[0] == 2:\n",
    "                topic2[en] = topic[1]\n",
    "            elif topic[0] == 3:\n",
    "                topic3[en] = topic[1]\n",
    "            elif topic[0] == 4:\n",
    "                topic3[en] = topic[1]\n",
    "            elif topic[0] == 5:\n",
    "                topic5[en] = topic[1]\n",
    "            elif topic[0] == 6:\n",
    "                topic6[en] = topic[1]\n",
    "            elif topic[0] == 7:\n",
    "                topic7[en] = topic[1]\n",
    "    X_train['topic0'] = topic0\n",
    "    X_train['topic1'] = topic1\n",
    "    X_train['topic2'] = topic2\n",
    "    X_train['topic3'] = topic3\n",
    "    X_train['topic4'] = topic4\n",
    "    X_train['topic5'] = topic5\n",
    "    X_train['topic6'] = topic6\n",
    "    X_train['topic7'] = topic7\n",
    "\n",
    "    # get testing distributions\n",
    "    distributions = lda_model_dvd[corp_dvd_test]\n",
    "    dvd_length = len(corp_dvd_test) \n",
    "    topic0 = [0] * dvd_length\n",
    "    topic1 = [0] * dvd_length\n",
    "    topic2 = [0] * dvd_length\n",
    "    topic3 = [0] * dvd_length\n",
    "    topic4 = [0] * dvd_length\n",
    "    topic5 = [0] * dvd_length\n",
    "    topic6 = [0] * dvd_length\n",
    "    topic7 = [0] * dvd_length\n",
    "    \n",
    "    # store the topic percentage values for testing\n",
    "    for en, row in enumerate(distributions):\n",
    "        topics = row[0]\n",
    "        for topic in topics:\n",
    "            if topic[0] == 0:\n",
    "                topic0[en] = topic[1]\n",
    "            elif topic[0] == 1:\n",
    "                topic1[en] = topic[1]\n",
    "            elif topic[0] == 2:\n",
    "                topic2[en] = topic[1]\n",
    "            elif topic[0] == 3:\n",
    "                topic3[en] = topic[1]\n",
    "            elif topic[0] == 4:\n",
    "                topic3[en] = topic[1]\n",
    "            elif topic[0] == 5:\n",
    "                topic5[en] = topic[1]\n",
    "            elif topic[0] == 6:\n",
    "                topic6[en] = topic[1]\n",
    "            elif topic[0] == 7:\n",
    "                topic7[en] = topic[1]\n",
    "    X_test['topic0'] = topic0\n",
    "    X_test['topic1'] = topic1\n",
    "    X_test['topic2'] = topic2\n",
    "    X_test['topic3'] = topic3\n",
    "    X_test['topic4'] = topic4\n",
    "    X_test['topic5'] = topic5\n",
    "    X_test['topic6'] = topic6\n",
    "    X_test['topic7'] = topic7\n",
    "\n",
    "    X_train = X_train[[\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"topic0\", \"topic1\", \"topic2\", \"topic3\", \"topic4\", \"topic5\", \"topic6\", \"topic7\"]]\n",
    "    X_test = X_test[[\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"topic0\", \"topic1\", \"topic2\", \"topic3\", \"topic4\", \"topic5\", \"topic6\", \"topic7\"]]\n",
    " \n",
    "    xgb = XGBClassifier(max_depth = max_depth, subsample = subsample)\n",
    "    y_pred_tr, y_pred_ts = predict_loo(xgb, X_train, y_train, X_test, y_test)\n",
    "    predictions_ts.append(y_pred_ts)\n",
    "    \n",
    "    # training\n",
    "    acc, prec, rec, auc = compute_metrics(y_train, y_pred_tr)\n",
    "    accuracy_tr.append(acc)\n",
    "    precision_tr.append(prec)\n",
    "    recall_tr.append(rec)\n",
    "    auc_tr.append(auc)\n",
    "    \n",
    "    print('finished round ', index)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:  0.984321785085144\n",
      "training precision:  0.9898246232554072\n",
      "training recall:  0.9412915679564798\n",
      "training auc:  0.9691988034023198\n",
      "testing accuracy:  0.7862595419847328\n",
      "testing precision:  0.5384615384615384\n",
      "testing recall:  0.4666666666666667\n",
      "testing auc  0.673927392739274\n"
     ]
    }
   ],
   "source": [
    "# training performance\n",
    "print('training accuracy: ', np.average(accuracy_tr))\n",
    "print('training precision: ', np.average(precision_tr))\n",
    "print('training recall: ', np.average(recall_tr))\n",
    "print('training auc: ', np.average(auc_tr))\n",
    "\n",
    "# testing performance\n",
    "accuracy, precision, recall, auc = compute_metrics(y, predictions_ts)\n",
    "print('testing accuracy: ', accuracy)\n",
    "print('testing precision: ', precision)\n",
    "print('testing recall: ', recall)\n",
    "print('testing auc ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline + Topics Performance \n",
    "\n",
    "Max_depth = 3, No_below = 0.1, No_above = 0.9, keep_n = 5000 (I get the same results for no_below = 0.3 or 0.5 since we're using 5000 words only)\n",
    "+ training accuracy:  0.9823840281855548, training precision:  0.9929346834519249, training recall:  0.9297973150829166, training auc:  0.963908180631172\n",
    "+ testing accuracy:  0.7938931297709924, testing precision:  0.5517241379310345, testing recall:  0.5333333333333333, testing auc  0.7023102310231023\n",
    "\n",
    "Max_depth = 3, No_below = 0.1, No_above = 0.9, keep_n = 7000 (I get the same results for no_below = 0.3 or 0.5 since we're using 5000 words only)\n",
    "+ training accuracy:  0.9768056371109807, training precision:  0.9787603136142227, training recall:  0.918961130121962, training auc:  0.9564705884908049\n",
    "+ testing accuracy:  0.7557251908396947, testing precision:  0.4642857142857143, testing recall:  0.43333333333333335, testing auc  0.6424092409240925\n",
    "\n",
    "Max_depth = 3, No_below = 0.2, No_above = 0.8, keep_n = 5000\n",
    "+ training accuracy:  0.990252495596007, training precision:  0.9957270458941564, training recall:  0.9614898657541459, training auc:  0.9801357574557141\n",
    "+ testing accuracy:  0.7557251908396947, testing precision:  0.4583333333333333, testing recall:  0.36666666666666664, testing auc  0.6189768976897689\n",
    "\n",
    "Max_depth = 3, Gamma = 3, No_below = 0.1, No_above = 0.9, keep_n = 5000\n",
    "+ training accuracy:  0.8428068115091015, training precision:  0.9189933457501935, training recall:  0.34485390892340095, training auc:  0.6677829366248021\n",
    "+ testing accuracy:  0.7938931297709924, testing precision:  0.6363636363636364, testing recall:  0.23333333333333334, testing auc  0.5968646864686469\n",
    "\n",
    "Max_depth = 3, Subsample = 0.7, No_below = 0.1, No_above = 0.9, keep_n = 5000\n",
    "+ training accuracy:  0.9717557251908397, training precision:  0.988359143252708, training recall:  0.8872247082565589, training auc:  0.9420512476359508\n",
    "+ testing accuracy:  0.7862595419847328, testing precision:  0.5357142857142857, testing recall:  0.5, testing auc  0.6856435643564357\n",
    "\n",
    "Max_depth = 3, Subsample = 0.9, No_below = 0.1, No_above = 0.9, keep_n = 5000\n",
    "+ training accuracy:  0.984321785085144, training precision:  0.9898246232554072, training recall:  0.9412915679564798, training auc:  0.9691988034023198\n",
    "+ testing accuracy:  0.7862595419847328, testing precision:  0.5384615384615384, testing recall:  0.4666666666666667, testing auc  0.673927392739274"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline + Advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dvd_advice_lda.csv')\n",
    "\n",
    "# Subset the dataframe\n",
    "lda_models = []\n",
    "factors_all = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\",\"Convo_1\", \"as1\", \"sur1\", \"rad1\"]\n",
    "factors_sub = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\",\"as1\", \"sur1\", \"rad1\"]\n",
    "df = df.dropna(subset=factors_all)\n",
    "X = df[factors_sub]\n",
    "y = df[\"txgot_binary\"]\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Run leave-one-out cross-validation\n",
    "predictions_ts = []\n",
    "accuracy_tr = []\n",
    "precision_tr = []\n",
    "recall_tr = []\n",
    "auc_tr = []\n",
    "max_depth = 3\n",
    "loo = LeaveOneOut()\n",
    "index = 1\n",
    "for train_index, test_index in loo.split(X):\n",
    "    #first split up the dataset\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = pd.DataFrame({'age':X_train[:,0],'gleason':X_train[:,1], \"DVD\": X_train[:,2], \"TxChoice2_orig\": X_train[:, 3], 'as1': X_train[:, 4], 'sur1': X_train[:, 5], 'rad1': X_train[:, 6]})\n",
    "    X_test = pd.DataFrame({'age':X_test[:,0],'gleason':X_test[:,1], \"DVD\": X_test[:,2], \"TxChoice2_orig\": X_test[:, 3], 'as1': X_test[:, 4], 'sur1': X_test[:, 5], 'rad1': X_test[:, 6]})\n",
    "    \n",
    "    X_train = X_train[[\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"as1\", \"sur1\", \"rad1\"]]\n",
    "    X_test = X_test[[\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\", \"as1\", \"sur1\", \"rad1\"]]\n",
    " \n",
    "    xgb = XGBClassifier(max_depth = max_depth)\n",
    "    y_pred_tr, y_pred_ts = predict_loo(xgb, X_train, y_train, X_test, y_test)\n",
    "    predictions_ts.append(y_pred_ts)\n",
    "    \n",
    "    # training\n",
    "    acc, prec, rec, auc = compute_metrics(y_train, y_pred_tr)\n",
    "    accuracy_tr.append(acc)\n",
    "    precision_tr.append(prec)\n",
    "    recall_tr.append(rec)\n",
    "    auc_tr.append(auc)\n",
    "    \n",
    "    print('finished round ', index)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training performance\n",
    "print('training accuracy: ', np.average(accuracy_tr))\n",
    "print('training precision: ', np.average(precision_tr))\n",
    "print('training recall: ', np.average(recall_tr))\n",
    "print('training auc: ', np.average(auc_tr))\n",
    "\n",
    "# testing performance\n",
    "accuracy, precision, recall, auc = compute_metrics(y, predictions_ts)\n",
    "print('testing accuracy: ', accuracy)\n",
    "print('testing precision: ', precision)\n",
    "print('testing recall: ', recall)\n",
    "print('testing auc ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline + Advice Performance\n",
    "\n",
    "Max_depth = 3\n",
    "+ training accuracy:  0.9619495008807987, training precision:  0.92926528531484, training recall:  0.9028428533824692, training auc:  0.941176169341072\n",
    "+ testing accuracy:  0.8091603053435115, testing precision:  0.5925925925925926, testing recall:  0.5333333333333333, testing auc  0.7122112211221121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dvd_advice_lda.csv')\n",
    "\n",
    "# Subset the dataframe\n",
    "lda_models = []\n",
    "factors_all = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\",\"Convo_1\", \"as1\", \"sur1\", \"rad1\"]\n",
    "factors_sub = [\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\"]\n",
    "df = df.dropna(subset=factors_all)\n",
    "X = df[factors_sub]\n",
    "y = df[\"txgot_binary\"]\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print('Shape :', X.shape)\n",
    "\n",
    "# Run leave-one-out cross-validation\n",
    "predictions_ts = []\n",
    "accuracy_tr = []\n",
    "precision_tr = []\n",
    "recall_tr = []\n",
    "auc_tr = []\n",
    "max_depth = 3\n",
    "loo = LeaveOneOut()\n",
    "index = 1\n",
    "for train_index, test_index in loo.split(X):\n",
    "    #first split up the dataset\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_train = pd.DataFrame({'age':X_train[:,0],'gleason':X_train[:,1], \"DVD\": X_train[:,2], \"TxChoice2_orig\" : X_train[:, 3]})\n",
    "    X_test = pd.DataFrame({'age':X_test[:,0],'gleason':X_test[:,1], \"DVD\": X_test[:,2], \"TxChoice2_orig\" : X_test[:, 3]})\n",
    "    \n",
    "    X_train = X_train[[\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\"]]\n",
    "    X_test = X_test[[\"age\", \"gleason\", \"DVD\", \"TxChoice2_orig\"]]\n",
    "    \n",
    "    xgb = XGBClassifier(max_depth = max_depth)\n",
    "    y_pred_tr, y_pred_ts = predict_loo(xgb, X_train, y_train, X_test, y_test)\n",
    "    predictions_ts.append(y_pred_ts)\n",
    "    \n",
    "    # training\n",
    "    acc, prec, rec, auc = compute_metrics(y_train, y_pred_tr)\n",
    "    accuracy_tr.append(acc)\n",
    "    precision_tr.append(prec)\n",
    "    recall_tr.append(rec)\n",
    "    auc_tr.append(auc)\n",
    "    \n",
    "    print('finished round ', index)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training performance\n",
    "print('training accuracy: ', np.average(accuracy_tr))\n",
    "print('training precision: ', np.average(precision_tr))\n",
    "print('training recall: ', np.average(recall_tr))\n",
    "print('training auc: ', np.average(auc_tr))\n",
    "\n",
    "# testing performance\n",
    "accuracy, precision, recall, auc = compute_metrics(y, predictions_ts)\n",
    "print('testing accuracy: ', accuracy)\n",
    "print('testing precision: ', precision)\n",
    "print('testing recall: ', recall)\n",
    "print('testing auc ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Performance\n",
    "\n",
    "Max_depth = 3\n",
    "+ training accuracy:  0.8826189078097477, training precision:  0.8552019941132032, training recall:  0.5868737387031676, training auc:  0.7786602084793897\n",
    "+ testing accuracy:  0.7938931297709924, testing precision:  0.56, testing recall:  0.4666666666666667, testing auc:  0.6788778877887789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
