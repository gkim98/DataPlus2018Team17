{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from ipywidgets import IntProgress\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# silences package warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df, target_var, cont_vars=[], cat_vars=[]):\n",
    "    total_vars = cont_vars + cat_vars + [target_var]\n",
    "    model_df = df[total_vars]\n",
    "    cleaned_df = model_df.dropna(subset=total_vars)\n",
    "\n",
    "    # turns categorical variables into dummy variables\n",
    "    for var in cat_vars:\n",
    "        temp_dummy = pd.get_dummies(cleaned_df[var], drop_first=True)\n",
    "        cleaned_df = pd.concat([cleaned_df.drop([var], axis=1), temp_dummy], axis=1)\n",
    "\n",
    "    # normalize the data\n",
    "    for var in cont_vars:\n",
    "        cleaned_df[var] = preprocessing.scale(cleaned_df[var])\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgbclassify(df, model, params=None, trainCV='none', target='txgot_binary', folds=5, iterations=100, print_id=1):\n",
    "    \n",
    "    feat_vars = [var for var in list(df.columns) if var != target]\n",
    "    X = df[feat_vars].values\n",
    "    y = df[target].values\n",
    "    \n",
    "    if trainCV=='grid':\n",
    "        param_bins = {key: {i:0 for i in params[key]} for key in params.keys()}\n",
    "    \n",
    "    avg_train_auc = 0\n",
    "    avg_test_auc = 0\n",
    "    avg_train_acc = 0\n",
    "    avg_test_acc = 0\n",
    "    \n",
    "    rskf = RepeatedStratifiedKFold(n_splits=folds, n_repeats=iterations)\n",
    "    for train_index, test_index in tqdm(rskf.split(X, y)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        train_auc, test_auc, train_acc, test_acc, best_params = xgbmodel(model, params, \n",
    "                                                                         X_train, X_test, \n",
    "                                                                         y_train, y_test, \n",
    "                                                                         trainCV, print_id)\n",
    "        \n",
    "        if trainCV=='grid':\n",
    "            for key, value in best_params.items():\n",
    "                param_bins[key][value] += 1\n",
    "        \n",
    "        avg_train_auc += train_auc\n",
    "        avg_test_auc += test_auc\n",
    "        avg_train_acc += train_acc\n",
    "        avg_test_acc += test_acc\n",
    "        \n",
    "    avg_train_auc /= (folds * iterations)\n",
    "    avg_test_auc /= (folds * iterations)\n",
    "    avg_train_acc /= (folds * iterations)\n",
    "    avg_test_acc /= (folds * iterations)\n",
    "    \n",
    "    print('Train vs. Test')\n",
    "    print('Training AUC: {}'.format(round(avg_train_auc, 3)))\n",
    "    print('Testing AUC: {}'.format(round(avg_test_auc, 3)))\n",
    "    print()\n",
    "    print('Training Accuracy: {}'.format(round(avg_train_acc, 3)))\n",
    "    print('Testing Accuracy: {}'.format(round(avg_test_acc, 3)))\n",
    "    \n",
    "    if trainCV=='grid':\n",
    "        print()\n",
    "        for key, value in param_bins.items():\n",
    "            print('{}: {}'.format(key, value))\n",
    "    \n",
    "    print()\n",
    "    print('Feature Importance:')\n",
    "    if not (trainCV is 'random' or trainCV is 'grid'):\n",
    "        sorted_idx = np.argsort(model.feature_importances_)[::-1]\n",
    "        for index in sorted_idx:\n",
    "            print([feat_vars[index], model.feature_importances_[index]])\n",
    "        \n",
    "    return avg_train_auc, avg_test_auc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgbmodel(model, params, X_train, X_test, y_train, y_test, trainCV, print_id):\n",
    "    if trainCV is 'random':\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle = True, random_state=print_id)\n",
    "        \n",
    "        X = np.concatenate((X_train, X_test), axis=0)\n",
    "        Y = np.concatenate((y_train, y_test), axis=0)\n",
    "        \n",
    "        my_model = RandomizedSearchCV(model, param_distributions=params, n_iter=5, \n",
    "                           scoring='roc_auc', n_jobs=4, \n",
    "                           cv=skf.split(X, Y), verbose=3)\n",
    "        \n",
    "        my_model.fit(X, Y)\n",
    "        best_params = my_model.best_params_\n",
    "        # print(best_params)\n",
    "        \n",
    "    elif trainCV is 'grid':\n",
    "        X = np.concatenate((X_train, X_test), axis=0)\n",
    "        Y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "        \n",
    "        my_model = GridSearchCV(model, params, scoring='roc_auc', cv=skf.split(X,Y))\n",
    "                \n",
    "        my_model.fit(X, Y)\n",
    "        best_params = my_model.best_params_\n",
    "        # print(best_params)\n",
    "        \n",
    "    else:\n",
    "        my_model = model\n",
    "        \n",
    "        my_model.fit(X_train, y_train)\n",
    "\n",
    "#         eval_set=[(X_train, y_train), (X_test, y_test)]\n",
    "#         my_model.fit(X_train, y_train, eval_set=eval_set, \n",
    "#                      eval_metric=\"auc\", early_stopping_rounds=15, verbose=False)\n",
    "        \n",
    "        best_params = params\n",
    "    \n",
    "    train_pred = my_model.predict(X_train)\n",
    "    test_pred = my_model.predict(X_test)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_train, train_pred, pos_label=1)\n",
    "    train_auc_score = auc(fpr, tpr)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, test_pred, pos_label=1)\n",
    "    test_auc_score = auc(fpr, tpr)\n",
    "    \n",
    "    train_auc = train_auc_score\n",
    "    test_auc = test_auc_score\n",
    "    \n",
    "    train_acc = my_model.score(X_train, y_train)\n",
    "    test_acc = my_model.score(X_test, y_test)\n",
    "    \n",
    "    test_metrics = confusion_matrix(y_test, test_pred).ravel()\n",
    "    train_metrics = confusion_matrix(y_train, train_pred).ravel()\n",
    "\n",
    "    return train_auc, test_auc, train_acc, test_acc, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataPlus",
   "language": "python",
   "name": "dataplus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
