{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Model with Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text features include the probabilities from text classification and sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_merged = pd.read_csv('../../DataPlus/processed_text_df.csv')\n",
    "merged = pd.read_csv('../../DataPlus/feature_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Decision Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\grant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import Preprocessing as pre\n",
    "import KFoldTextClassification as ktc\n",
    "import CompilingCorpus as cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_values(df):\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=['Convo_1', 'txgot_binary'])\n",
    "    \n",
    "    decision_values, _ = ktc.strat_kfold_text(df, folds=2, iterations=5)\n",
    "    df['decision_values'] = decision_values\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc7cf1493e24e61b94cf8093acc6454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000001914C165F60, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\g...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000001914C165F60, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\g...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'text_df = add_text_values(processed_merged)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 6, 28, 15, 52, 22, 175865, tzinfo=tzutc()), 'msg_id': '2f21549c46af4672833f97884ccb1ff2', 'msg_type': 'execute_request', 'session': '7b8236800f6941ddbb0ba81f459a2e10', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '2f21549c46af4672833f97884ccb1ff2', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'7b8236800f6941ddbb0ba81f459a2e10']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'text_df = add_text_values(processed_merged)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 6, 28, 15, 52, 22, 175865, tzinfo=tzutc()), 'msg_id': '2f21549c46af4672833f97884ccb1ff2', 'msg_type': 'execute_request', 'session': '7b8236800f6941ddbb0ba81f459a2e10', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '2f21549c46af4672833f97884ccb1ff2', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'7b8236800f6941ddbb0ba81f459a2e10'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'text_df = add_text_values(processed_merged)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 6, 28, 15, 52, 22, 175865, tzinfo=tzutc()), 'msg_id': '2f21549c46af4672833f97884ccb1ff2', 'msg_type': 'execute_request', 'session': '7b8236800f6941ddbb0ba81f459a2e10', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '2f21549c46af4672833f97884ccb1ff2', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='text_df = add_text_values(processed_merged)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'text_df = add_text_values(processed_merged)'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('text_df = add_text_values(processed_merged)',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('text_df = add_text_values(processed_merged)',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='text_df = add_text_values(processed_merged)', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>], cell_name='<ipython-input-5-4a3a621f40b3>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 19155c059b0, executio..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x0000019155968C00, file \"<ipython-input-5-4a3a621f40b3>\", line 1>\n        result = <ExecutionResult object at 19155c059b0, executio..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x0000019155968C00, file \"<ipython-input-5-4a3a621f40b3>\", line 1>, result=<ExecutionResult object at 19155c059b0, executio..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x0000019155968C00, file \"<ipython-input-5-4a3a621f40b3>\", line 1>\n        self.user_global_ns = {'In': ['', 'import pandas as pd', \"processed_merged = pd.read_csv('../../DataPlus/p....read_csv('../../DataPlus/feature_dataframe.csv')\", 'import Preprocessing as pre\\nimport KFoldTextClassification as ktc\\nimport CompilingCorpus as cc', \"def add_text_values(df):\\n    df = df.copy()\\n    ...ion_values'] = decision_values\\n    \\n    return df\", 'text_df = add_text_values(processed_merged)'], 'Out': {}, '_': '', '__': '', '___': '', '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '__doc__': 'Automatically created module for IPython interactive environment', '__loader__': None, '__name__': '__main__', ...}\n        self.user_ns = {'In': ['', 'import pandas as pd', \"processed_merged = pd.read_csv('../../DataPlus/p....read_csv('../../DataPlus/feature_dataframe.csv')\", 'import Preprocessing as pre\\nimport KFoldTextClassification as ktc\\nimport CompilingCorpus as cc', \"def add_text_values(df):\\n    df = df.copy()\\n    ...ion_values'] = decision_values\\n    \\n    return df\", 'text_df = add_text_values(processed_merged)'], 'Out': {}, '_': '', '__': '', '___': '', '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '__doc__': 'Automatically created module for IPython interactive environment', '__loader__': None, '__name__': '__main__', ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\grant\\DataFinal\\gk_scripts\\<ipython-input-5-4a3a621f40b3> in <module>()\n----> 1 text_df = add_text_values(processed_merged)\n\n...........................................................................\nC:\\Users\\grant\\DataFinal\\gk_scripts\\<ipython-input-4-10fb2042af3e> in add_text_values(df=     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 Ad...  White       Married  \n\n[356 rows x 881 columns])\n      1 def add_text_values(df):\n      2     df = df.copy()\n      3     df = df.dropna(subset=['Convo_1', 'txgot_binary'])\n      4     \n----> 5     decision_values, _ = ktc.strat_kfold_text(df, folds=2, iterations=5)\n      6     df['decision_values'] = decision_values\n      7     \n      8     return df\n\n...........................................................................\nC:\\Users\\grant\\DataFinal\\gk_scripts\\KFoldTextClassification.py in strat_kfold_text(df=     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 Ad...  White       Married  \n\n[356 rows x 881 columns], grid_search=True, hyp_params={'clf__alpha': (0.01, 0.001), 'tfidf__use_idf': (True, False), 'vect__ngram_range': [(1, 1), (1, 2)]}, folds=2, iterations=5, print_results=True)\n     40 \n     41     rskf = RepeatedStratifiedKFold(n_splits=folds, n_repeats=iterations)\n     42     for train_index, test_index in tqdm(rskf.split(X, y)):\n     43         X_train, X_test = X[train_index], X[test_index]\n     44         y_train, y_test = y[train_index], y[test_index]\n---> 45         (tn, fp, fn, tp), decision_function = train_text(X_train, y_train, X_test, y_test, grid_search, hyp_params)\n        tn = undefined\n        fp = undefined\n        fn = undefined\n        tp = undefined\n        decision_function = undefined\n        X_train = array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y_train = array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.])\n        X_test = array([\"['l e g e n d', 'r e c o r d e r', 'o t ...e c o r d i n g', 'e n d']\"],\n      dtype=object)\n        y_test = array([0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0..., 0.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0.])\n        grid_search = True\n        hyp_params = {'clf__alpha': (0.01, 0.001), 'tfidf__use_idf': (True, False), 'vect__ngram_range': [(1, 1), (1, 2)]}\n     46 \n     47         # aggregating decision function values\n     48         dec_result = decision_function(X_test)\n     49         for i in range(len(test_index)):\n\n...........................................................................\nC:\\Users\\grant\\DataFinal\\gk_scripts\\KFoldTextClassification.py in train_text(X_train=array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y_train=array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), X_test=array([\"['l e g e n d', 'r e c o r d e r', 'o t ...e c o r d i n g', 'e n d']\"],\n      dtype=object), y_test=array([0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0..., 0.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0.]), grid_search=True, hyp_params={'clf__alpha': (0.01, 0.001), 'tfidf__use_idf': (True, False), 'vect__ngram_range': [(1, 1), (1, 2)]})\n     90         ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None))\n     91     ])\n     92 \n     93     if grid_search:\n     94         model = GridSearchCV(text_clf, hyp_params, n_jobs=-1)\n---> 95         model = model.fit(X_train, y_train)\n        model = GridSearchCV(cv=None, error_score='raise',\n     ...ain_score='warn',\n       scoring=None, verbose=0)\n        model.fit = <bound method BaseSearchCV.fit of GridSearchCV(c...in_score='warn',\n       scoring=None, verbose=0)>\n        X_train = array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y_train = array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.])\n     96     else:\n     97         model = text_clf.fit(X_train, y_train)\n     98 \n     99     predictions = model.predict(X_test)\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...ain_score='warn',\n       scoring=None, verbose=0), X=array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=3, random_state=None, shuffle=False)>\n        X = array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y = array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.])\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Thu Jun 28 11:52:23 2018\nPID: 13092                Python 3.6.4: C:\\Users\\grant\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), {'score': <function _passthrough_scorer>}, array([ 55,  57,  62,  63,  64,  65,  66,  67,  ...67, 168, 169, 170, 171, 172, 173, 174, 175, 176]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 50,\n       51, 52, 53, 54, 56, 58, 59, 60, 61]), 0, {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), {'score': <function _passthrough_scorer>}, array([ 55,  57,  62,  63,  64,  65,  66,  67,  ...67, 168, 169, 170, 171, 172, 173, 174, 175, 176]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 50,\n       51, 52, 53, 54, 56, 58, 59, 60, 61]), 0, {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), X=array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), scorer={'score': <function _passthrough_scorer>}, train=array([ 55,  57,  62,  63,  64,  65,  66,  67,  ...67, 168, 169, 170, 171, 172, 173, 174, 175, 176]), test=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 50,\n       51, 52, 53, 54, 56, 58, 59, 60, 61]), verbose=0, parameters={'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...       tol=None, verbose=0, warm_start=False))])>\n        X_train = array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y_train = array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), X=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), **fit_params={})\n    243         Returns\n    244         -------\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n--> 248         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(memory=N...       tol=None, verbose=0, warm_start=False))])>\n        X = array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y = array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n    249         if self._final_estimator is not None:\n    250             self._final_estimator.fit(Xt, y, **fit_params)\n    251         return self\n    252 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in _fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), X=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), **fit_params={})\n    208                 else:\n    209                     cloned_transformer = clone(transformer)\n    210                 # Fit or load from cache the current transfomer\n    211                 Xt, fitted_transformer = fit_transform_one_cached(\n    212                     cloned_transformer, None, Xt, y,\n--> 213                     **fit_params_steps[name])\n        fit_params_steps = {'clf': {}, 'tfidf': {}, 'vect': {}}\n        name = 'vect'\n    214                 # Replace the transformer of the step with the fitted\n    215                 # transformer. This is necessary when loading the transformer\n    216                 # from the cache.\n    217                 self.steps[step_idx] = (name, fitted_transformer)\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py in __call__(self=NotMemorizedFunc(func=<function _fit_transform_one at 0x0000027CBD185D08>), *args=(CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), **kwargs={})\n    357     # Should be a light as possible (for speed)\n    358     def __init__(self, func):\n    359         self.func = func\n    360 \n    361     def __call__(self, *args, **kwargs):\n--> 362         return self.func(*args, **kwargs)\n        self.func = <function _fit_transform_one>\n        args = (CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n        kwargs = {}\n    363 \n    364     def call_and_shelve(self, *args, **kwargs):\n    365         return NotMemorizedResult(self.func(*args, **kwargs))\n    366 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in _fit_transform_one(transformer=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), weight=None, X=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), **fit_params={})\n    576 \n    577 \n    578 def _fit_transform_one(transformer, weight, X, y,\n    579                        **fit_params):\n    580     if hasattr(transformer, 'fit_transform'):\n--> 581         res = transformer.fit_transform(X, y, **fit_params)\n        res = undefined\n        transformer.fit_transform = <bound method CountVectorizer.fit_transform of C...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        X = array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y = array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n        fit_params = {}\n    582     else:\n    583         res = transformer.fit(X, y, **fit_params).transform(X)\n    584     # if we have a weight for this transformer, multiply output\n    585     if weight is None:\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n    864         max_df = self.max_df\n    865         min_df = self.min_df\n    866         max_features = self.max_features\n    867 \n    868         vocabulary, X = self._count_vocab(raw_documents,\n--> 869                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in _count_vocab(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), fixed_vocab=False)\n    806 \n    807         if not fixed_vocab:\n    808             # disable defaultdict behaviour\n    809             vocabulary = dict(vocabulary)\n    810             if not vocabulary:\n--> 811                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n    812                                  \" contain stop words\")\n    813 \n    814         j_indices = np.asarray(j_indices, dtype=np.intc)\n    815         indptr = np.frombuffer(indptr, dtype=np.intc)\n\nValueError: empty vocabulary; perhaps the documents only contain stop words\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 458, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 248, in fit\n    Xt, fit_params = self._fit(X, y, **fit_params)\n  File \"C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 213, in _fit\n    **fit_params_steps[name])\n  File \"C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\", line 362, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 581, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 869, in fit_transform\n    self.fixed_vocabulary_)\n  File \"C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 811, in _count_vocab\n    raise ValueError(\"empty vocabulary; perhaps the documents only\"\nValueError: empty vocabulary; perhaps the documents only contain stop words\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\grant\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nValueError                                         Thu Jun 28 11:52:23 2018\nPID: 13092                Python 3.6.4: C:\\Users\\grant\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), {'score': <function _passthrough_scorer>}, array([ 55,  57,  62,  63,  64,  65,  66,  67,  ...67, 168, 169, 170, 171, 172, 173, 174, 175, 176]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 50,\n       51, 52, 53, 54, 56, 58, 59, 60, 61]), 0, {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), {'score': <function _passthrough_scorer>}, array([ 55,  57,  62,  63,  64,  65,  66,  67,  ...67, 168, 169, 170, 171, 172, 173, 174, 175, 176]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 50,\n       51, 52, 53, 54, 56, 58, 59, 60, 61]), 0, {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), X=array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), scorer={'score': <function _passthrough_scorer>}, train=array([ 55,  57,  62,  63,  64,  65,  66,  67,  ...67, 168, 169, 170, 171, 172, 173, 174, 175, 176]), test=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 50,\n       51, 52, 53, 54, 56, 58, 59, 60, 61]), verbose=0, parameters={'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...       tol=None, verbose=0, warm_start=False))])>\n        X_train = array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y_train = array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), X=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), **fit_params={})\n    243         Returns\n    244         -------\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n--> 248         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(memory=N...       tol=None, verbose=0, warm_start=False))])>\n        X = array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y = array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n    249         if self._final_estimator is not None:\n    250             self._final_estimator.fit(Xt, y, **fit_params)\n    251         return self\n    252 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in _fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), X=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), **fit_params={})\n    208                 else:\n    209                     cloned_transformer = clone(transformer)\n    210                 # Fit or load from cache the current transfomer\n    211                 Xt, fitted_transformer = fit_transform_one_cached(\n    212                     cloned_transformer, None, Xt, y,\n--> 213                     **fit_params_steps[name])\n        fit_params_steps = {'clf': {}, 'tfidf': {}, 'vect': {}}\n        name = 'vect'\n    214                 # Replace the transformer of the step with the fitted\n    215                 # transformer. This is necessary when loading the transformer\n    216                 # from the cache.\n    217                 self.steps[step_idx] = (name, fitted_transformer)\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py in __call__(self=NotMemorizedFunc(func=<function _fit_transform_one at 0x0000027CBD185D08>), *args=(CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), **kwargs={})\n    357     # Should be a light as possible (for speed)\n    358     def __init__(self, func):\n    359         self.func = func\n    360 \n    361     def __call__(self, *args, **kwargs):\n--> 362         return self.func(*args, **kwargs)\n        self.func = <function _fit_transform_one>\n        args = (CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n        kwargs = {}\n    363 \n    364     def call_and_shelve(self, *args, **kwargs):\n    365         return NotMemorizedResult(self.func(*args, **kwargs))\n    366 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in _fit_transform_one(transformer=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), weight=None, X=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), **fit_params={})\n    576 \n    577 \n    578 def _fit_transform_one(transformer, weight, X, y,\n    579                        **fit_params):\n    580     if hasattr(transformer, 'fit_transform'):\n--> 581         res = transformer.fit_transform(X, y, **fit_params)\n        res = undefined\n        transformer.fit_transform = <bound method CountVectorizer.fit_transform of C...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        X = array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y = array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n        fit_params = {}\n    582     else:\n    583         res = transformer.fit(X, y, **fit_params).transform(X)\n    584     # if we have a weight for this transformer, multiply output\n    585     if weight is None:\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n    864         max_df = self.max_df\n    865         min_df = self.min_df\n    866         max_features = self.max_features\n    867 \n    868         vocabulary, X = self._count_vocab(raw_documents,\n--> 869                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in _count_vocab(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), fixed_vocab=False)\n    806 \n    807         if not fixed_vocab:\n    808             # disable defaultdict behaviour\n    809             vocabulary = dict(vocabulary)\n    810             if not vocabulary:\n--> 811                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n    812                                  \" contain stop words\")\n    813 \n    814         j_indices = np.asarray(j_indices, dtype=np.intc)\n    815         indptr = np.frombuffer(indptr, dtype=np.intc)\n\nValueError: empty vocabulary; perhaps the documents only contain stop words\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nValueError                                         Thu Jun 28 11:52:23 2018\nPID: 13092                Python 3.6.4: C:\\Users\\grant\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), {'score': <function _passthrough_scorer>}, array([ 55,  57,  62,  63,  64,  65,  66,  67,  ...67, 168, 169, 170, 171, 172, 173, 174, 175, 176]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 50,\n       51, 52, 53, 54, 56, 58, 59, 60, 61]), 0, {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), {'score': <function _passthrough_scorer>}, array([ 55,  57,  62,  63,  64,  65,  66,  67,  ...67, 168, 169, 170, 171, 172, 173, 174, 175, 176]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 50,\n       51, 52, 53, 54, 56, 58, 59, 60, 61]), 0, {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), X=array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), scorer={'score': <function _passthrough_scorer>}, train=array([ 55,  57,  62,  63,  64,  65,  66,  67,  ...67, 168, 169, 170, 171, 172, 173, 174, 175, 176]), test=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 50,\n       51, 52, 53, 54, 56, 58, 59, 60, 61]), verbose=0, parameters={'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...       tol=None, verbose=0, warm_start=False))])>\n        X_train = array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y_train = array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), X=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), **fit_params={})\n    243         Returns\n    244         -------\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n--> 248         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(memory=N...       tol=None, verbose=0, warm_start=False))])>\n        X = array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y = array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n    249         if self._final_estimator is not None:\n    250             self._final_estimator.fit(Xt, y, **fit_params)\n    251         return self\n    252 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in _fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), X=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), **fit_params={})\n    208                 else:\n    209                     cloned_transformer = clone(transformer)\n    210                 # Fit or load from cache the current transfomer\n    211                 Xt, fitted_transformer = fit_transform_one_cached(\n    212                     cloned_transformer, None, Xt, y,\n--> 213                     **fit_params_steps[name])\n        fit_params_steps = {'clf': {}, 'tfidf': {}, 'vect': {}}\n        name = 'vect'\n    214                 # Replace the transformer of the step with the fitted\n    215                 # transformer. This is necessary when loading the transformer\n    216                 # from the cache.\n    217                 self.steps[step_idx] = (name, fitted_transformer)\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py in __call__(self=NotMemorizedFunc(func=<function _fit_transform_one at 0x0000027CBD185D08>), *args=(CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), **kwargs={})\n    357     # Should be a light as possible (for speed)\n    358     def __init__(self, func):\n    359         self.func = func\n    360 \n    361     def __call__(self, *args, **kwargs):\n--> 362         return self.func(*args, **kwargs)\n        self.func = <function _fit_transform_one>\n        args = (CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n        kwargs = {}\n    363 \n    364     def call_and_shelve(self, *args, **kwargs):\n    365         return NotMemorizedResult(self.func(*args, **kwargs))\n    366 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in _fit_transform_one(transformer=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), weight=None, X=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), **fit_params={})\n    576 \n    577 \n    578 def _fit_transform_one(transformer, weight, X, y,\n    579                        **fit_params):\n    580     if hasattr(transformer, 'fit_transform'):\n--> 581         res = transformer.fit_transform(X, y, **fit_params)\n        res = undefined\n        transformer.fit_transform = <bound method CountVectorizer.fit_transform of C...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        X = array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y = array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n        fit_params = {}\n    582     else:\n    583         res = transformer.fit(X, y, **fit_params).transform(X)\n    584     # if we have a weight for this transformer, multiply output\n    585     if weight is None:\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n    864         max_df = self.max_df\n    865         min_df = self.min_df\n    866         max_features = self.max_features\n    867 \n    868         vocabulary, X = self._count_vocab(raw_documents,\n--> 869                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in _count_vocab(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), fixed_vocab=False)\n    806 \n    807         if not fixed_vocab:\n    808             # disable defaultdict behaviour\n    809             vocabulary = dict(vocabulary)\n    810             if not vocabulary:\n--> 811                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n    812                                  \" contain stop words\")\n    813 \n    814         j_indices = np.asarray(j_indices, dtype=np.intc)\n    815         indptr = np.frombuffer(indptr, dtype=np.intc)\n\nValueError: empty vocabulary; perhaps the documents only contain stop words\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4a3a621f40b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_text_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_merged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-10fb2042af3e>\u001b[0m in \u001b[0;36madd_text_values\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Convo_1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'txgot_binary'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdecision_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mktc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrat_kfold_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'decision_values'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecision_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DataFinal\\gk_scripts\\KFoldTextClassification.py\u001b[0m in \u001b[0;36mstrat_kfold_text\u001b[1;34m(df, grid_search, hyp_params, folds, iterations, print_results)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mtn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecision_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyp_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# aggregating decision function values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\DataFinal\\gk_scripts\\KFoldTextClassification.py\u001b[0m in \u001b[0;36mtrain_text\u001b[1;34m(X_train, y_train, X_test, y_test, grid_search, hyp_params)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyp_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000001914C165F60, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\g...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000001914C165F60, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\g...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    481         if self.poller is not None:\n    482             self.poller.start()\n    483         self.kernel.start()\n    484         self.io_loop = ioloop.IOLoop.current()\n    485         try:\n--> 486             self.io_loop.start()\n        self.io_loop.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    487         except KeyboardInterrupt:\n    488             pass\n    489 \n    490 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'text_df = add_text_values(processed_merged)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 6, 28, 15, 52, 22, 175865, tzinfo=tzutc()), 'msg_id': '2f21549c46af4672833f97884ccb1ff2', 'msg_type': 'execute_request', 'session': '7b8236800f6941ddbb0ba81f459a2e10', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '2f21549c46af4672833f97884ccb1ff2', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'7b8236800f6941ddbb0ba81f459a2e10']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'text_df = add_text_values(processed_merged)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 6, 28, 15, 52, 22, 175865, tzinfo=tzutc()), 'msg_id': '2f21549c46af4672833f97884ccb1ff2', 'msg_type': 'execute_request', 'session': '7b8236800f6941ddbb0ba81f459a2e10', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '2f21549c46af4672833f97884ccb1ff2', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'7b8236800f6941ddbb0ba81f459a2e10'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'text_df = add_text_values(processed_merged)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 6, 28, 15, 52, 22, 175865, tzinfo=tzutc()), 'msg_id': '2f21549c46af4672833f97884ccb1ff2', 'msg_type': 'execute_request', 'session': '7b8236800f6941ddbb0ba81f459a2e10', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '2f21549c46af4672833f97884ccb1ff2', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='text_df = add_text_values(processed_merged)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'text_df = add_text_values(processed_merged)'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('text_df = add_text_values(processed_merged)',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('text_df = add_text_values(processed_merged)',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='text_df = add_text_values(processed_merged)', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>], cell_name='<ipython-input-5-4a3a621f40b3>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 19155c059b0, executio..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x0000019155968C00, file \"<ipython-input-5-4a3a621f40b3>\", line 1>\n        result = <ExecutionResult object at 19155c059b0, executio..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x0000019155968C00, file \"<ipython-input-5-4a3a621f40b3>\", line 1>, result=<ExecutionResult object at 19155c059b0, executio..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x0000019155968C00, file \"<ipython-input-5-4a3a621f40b3>\", line 1>\n        self.user_global_ns = {'In': ['', 'import pandas as pd', \"processed_merged = pd.read_csv('../../DataPlus/p....read_csv('../../DataPlus/feature_dataframe.csv')\", 'import Preprocessing as pre\\nimport KFoldTextClassification as ktc\\nimport CompilingCorpus as cc', \"def add_text_values(df):\\n    df = df.copy()\\n    ...ion_values'] = decision_values\\n    \\n    return df\", 'text_df = add_text_values(processed_merged)'], 'Out': {}, '_': '', '__': '', '___': '', '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '__doc__': 'Automatically created module for IPython interactive environment', '__loader__': None, '__name__': '__main__', ...}\n        self.user_ns = {'In': ['', 'import pandas as pd', \"processed_merged = pd.read_csv('../../DataPlus/p....read_csv('../../DataPlus/feature_dataframe.csv')\", 'import Preprocessing as pre\\nimport KFoldTextClassification as ktc\\nimport CompilingCorpus as cc', \"def add_text_values(df):\\n    df = df.copy()\\n    ...ion_values'] = decision_values\\n    \\n    return df\", 'text_df = add_text_values(processed_merged)'], 'Out': {}, '_': '', '__': '', '___': '', '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '__doc__': 'Automatically created module for IPython interactive environment', '__loader__': None, '__name__': '__main__', ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\grant\\DataFinal\\gk_scripts\\<ipython-input-5-4a3a621f40b3> in <module>()\n----> 1 text_df = add_text_values(processed_merged)\n\n...........................................................................\nC:\\Users\\grant\\DataFinal\\gk_scripts\\<ipython-input-4-10fb2042af3e> in add_text_values(df=     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 Ad...  White       Married  \n\n[356 rows x 881 columns])\n      1 def add_text_values(df):\n      2     df = df.copy()\n      3     df = df.dropna(subset=['Convo_1', 'txgot_binary'])\n      4     \n----> 5     decision_values, _ = ktc.strat_kfold_text(df, folds=2, iterations=5)\n      6     df['decision_values'] = decision_values\n      7     \n      8     return df\n\n...........................................................................\nC:\\Users\\grant\\DataFinal\\gk_scripts\\KFoldTextClassification.py in strat_kfold_text(df=     Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 Ad...  White       Married  \n\n[356 rows x 881 columns], grid_search=True, hyp_params={'clf__alpha': (0.01, 0.001), 'tfidf__use_idf': (True, False), 'vect__ngram_range': [(1, 1), (1, 2)]}, folds=2, iterations=5, print_results=True)\n     40 \n     41     rskf = RepeatedStratifiedKFold(n_splits=folds, n_repeats=iterations)\n     42     for train_index, test_index in tqdm(rskf.split(X, y)):\n     43         X_train, X_test = X[train_index], X[test_index]\n     44         y_train, y_test = y[train_index], y[test_index]\n---> 45         (tn, fp, fn, tp), decision_function = train_text(X_train, y_train, X_test, y_test, grid_search, hyp_params)\n        tn = undefined\n        fp = undefined\n        fn = undefined\n        tp = undefined\n        decision_function = undefined\n        X_train = array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y_train = array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.])\n        X_test = array([\"['l e g e n d', 'r e c o r d e r', 'o t ...e c o r d i n g', 'e n d']\"],\n      dtype=object)\n        y_test = array([0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0..., 0.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0.])\n        grid_search = True\n        hyp_params = {'clf__alpha': (0.01, 0.001), 'tfidf__use_idf': (True, False), 'vect__ngram_range': [(1, 1), (1, 2)]}\n     46 \n     47         # aggregating decision function values\n     48         dec_result = decision_function(X_test)\n     49         for i in range(len(test_index)):\n\n...........................................................................\nC:\\Users\\grant\\DataFinal\\gk_scripts\\KFoldTextClassification.py in train_text(X_train=array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y_train=array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), X_test=array([\"['l e g e n d', 'r e c o r d e r', 'o t ...e c o r d i n g', 'e n d']\"],\n      dtype=object), y_test=array([0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0..., 0.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0.]), grid_search=True, hyp_params={'clf__alpha': (0.01, 0.001), 'tfidf__use_idf': (True, False), 'vect__ngram_range': [(1, 1), (1, 2)]})\n     90         ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None))\n     91     ])\n     92 \n     93     if grid_search:\n     94         model = GridSearchCV(text_clf, hyp_params, n_jobs=-1)\n---> 95         model = model.fit(X_train, y_train)\n        model = GridSearchCV(cv=None, error_score='raise',\n     ...ain_score='warn',\n       scoring=None, verbose=0)\n        model.fit = <bound method BaseSearchCV.fit of GridSearchCV(c...in_score='warn',\n       scoring=None, verbose=0)>\n        X_train = array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y_train = array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.])\n     96     else:\n     97         model = text_clf.fit(X_train, y_train)\n     98 \n     99     predictions = model.predict(X_test)\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...ain_score='warn',\n       scoring=None, verbose=0), X=array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...ld(n_splits=3, random_state=None, shuffle=False)>\n        X = array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y = array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.])\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Thu Jun 28 11:52:23 2018\nPID: 13092                Python 3.6.4: C:\\Users\\grant\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), {'score': <function _passthrough_scorer>}, array([ 55,  57,  62,  63,  64,  65,  66,  67,  ...67, 168, 169, 170, 171, 172, 173, 174, 175, 176]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 50,\n       51, 52, 53, 54, 56, 58, 59, 60, 61]), 0, {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), {'score': <function _passthrough_scorer>}, array([ 55,  57,  62,  63,  64,  65,  66,  67,  ...67, 168, 169, 170, 171, 172, 173, 174, 175, 176]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 50,\n       51, 52, 53, 54, 56, 58, 59, 60, 61]), 0, {'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), X=array([\"['l e g e n d', 't h a n k', 'c o m i n ... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1..., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0.]), scorer={'score': <function _passthrough_scorer>}, train=array([ 55,  57,  62,  63,  64,  65,  66,  67,  ...67, 168, 169, 170, 171, 172, 173, 174, 175, 176]), test=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 50,\n       51, 52, 53, 54, 56, 58, 59, 60, 61]), verbose=0, parameters={'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(memory=No...       tol=None, verbose=0, warm_start=False))])>\n        X_train = array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y_train = array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), X=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), **fit_params={})\n    243         Returns\n    244         -------\n    245         self : Pipeline\n    246             This estimator\n    247         \"\"\"\n--> 248         Xt, fit_params = self._fit(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._fit = <bound method Pipeline._fit of Pipeline(memory=N...       tol=None, verbose=0, warm_start=False))])>\n        X = array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y = array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n    249         if self._final_estimator is not None:\n    250             self._final_estimator.fit(Xt, y, **fit_params)\n    251         return self\n    252 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in _fit(self=Pipeline(memory=None,\n     steps=[('vect', Count...\n       tol=None, verbose=0, warm_start=False))]), X=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), **fit_params={})\n    208                 else:\n    209                     cloned_transformer = clone(transformer)\n    210                 # Fit or load from cache the current transfomer\n    211                 Xt, fitted_transformer = fit_transform_one_cached(\n    212                     cloned_transformer, None, Xt, y,\n--> 213                     **fit_params_steps[name])\n        fit_params_steps = {'clf': {}, 'tfidf': {}, 'vect': {}}\n        name = 'vect'\n    214                 # Replace the transformer of the step with the fitted\n    215                 # transformer. This is necessary when loading the transformer\n    216                 # from the cache.\n    217                 self.steps[step_idx] = (name, fitted_transformer)\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py in __call__(self=NotMemorizedFunc(func=<function _fit_transform_one at 0x0000027CBD185D08>), *args=(CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), **kwargs={})\n    357     # Should be a light as possible (for speed)\n    358     def __init__(self, func):\n    359         self.func = func\n    360 \n    361     def __call__(self, *args, **kwargs):\n--> 362         return self.func(*args, **kwargs)\n        self.func = <function _fit_transform_one>\n        args = (CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), None, array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n        kwargs = {}\n    363 \n    364     def call_and_shelve(self, *args, **kwargs):\n    365         return NotMemorizedResult(self.func(*args, **kwargs))\n    366 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py in _fit_transform_one(transformer=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), weight=None, X=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), **fit_params={})\n    576 \n    577 \n    578 def _fit_transform_one(transformer, weight, X, y,\n    579                        **fit_params):\n    580     if hasattr(transformer, 'fit_transform'):\n--> 581         res = transformer.fit_transform(X, y, **fit_params)\n        res = undefined\n        transformer.fit_transform = <bound method CountVectorizer.fit_transform of C...w+\\\\b',\n        tokenizer=None, vocabulary=None)>\n        X = array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object)\n        y = array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n        fit_params = {}\n    582     else:\n    583         res = transformer.fit(X, y, **fit_params).transform(X)\n    584     # if we have a weight for this transformer, multiply output\n    585     if weight is None:\n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in fit_transform(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), y=array([1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0... 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n    864         max_df = self.max_df\n    865         min_df = self.min_df\n    866         max_features = self.max_features\n    867 \n    868         vocabulary, X = self._count_vocab(raw_documents,\n--> 869                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n\n...........................................................................\nC:\\Users\\grant\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py in _count_vocab(self=CountVectorizer(analyzer='word', binary=False, d...\\w+\\\\b',\n        tokenizer=None, vocabulary=None), raw_documents=array([\"['l e g e n d', 'g o o d', 'n e w s', 'b... e s', 'l o o k', 'w a y']\"],\n      dtype=object), fixed_vocab=False)\n    806 \n    807         if not fixed_vocab:\n    808             # disable defaultdict behaviour\n    809             vocabulary = dict(vocabulary)\n    810             if not vocabulary:\n--> 811                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n    812                                  \" contain stop words\")\n    813 \n    814         j_indices = np.asarray(j_indices, dtype=np.intc)\n    815         indptr = np.frombuffer(indptr, dtype=np.intc)\n\nValueError: empty vocabulary; perhaps the documents only contain stop words\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "text_df = add_text_values(processed_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['Convo_1'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_merged['Convo_1'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(processed_merged['Convo_1'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
